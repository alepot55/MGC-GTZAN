{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c86d0bee",
   "metadata": {},
   "source": [
    "# Notebook 00: Setup e Preparazione Dati GTZAN\n",
    "\n",
    "**Scopo:** Questo notebook ha il compito di caricare il dataset GTZAN originale, applicare tutto il pre-processing necessario (segmentazione, estrazione di spettrogrammi, normalizzazione) e salvare su disco un set di dati pulito e pronto per l'addestramento.\n",
    "\n",
    "**Questo notebook va eseguito una sola volta.**\n",
    "\n",
    "**Input:**\n",
    "- Cartella con i file audio originali: `../data/gtzan/genres/`\n",
    "\n",
    "**Output (salvati in `../data/processed/`):**\n",
    "- Array NumPy: `X_train.npy`, `y_train.npy`, `X_val.npy`, `y_val.npy`, `X_test.npy`, `y_test.npy`\n",
    "- Artefatti di pre-processing: `scaler.pkl`, `label_encoder.pkl`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "487a3d03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Ambiente pronto per la preparazione dei dati.\n"
     ]
    }
   ],
   "source": [
    "# ===================================================================\n",
    "# CELLA 1: SETUP, IMPORTS E CONFIGURAZIONE GLOBALE\n",
    "# ===================================================================\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import librosa\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# --- Global Constants ---\n",
    "DATA_PATH = '../data/gtzan/genres/'\n",
    "PROCESSED_DATA_PATH = '../data/processed/'\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "# --- Creazione Cartella di Output ---\n",
    "os.makedirs(PROCESSED_DATA_PATH, exist_ok=True)\n",
    "\n",
    "print(\"âœ… Ambiente pronto per la preparazione dei dati.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6fa44d4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Classe GTZANDataLoader definita e corretta.\n"
     ]
    }
   ],
   "source": [
    "# ===================================================================\n",
    "# CELLA 2: CLASSE DATALOADER (CORRETTA E POTENZIATA)\n",
    "# ===================================================================\n",
    "\n",
    "class GTZANDataLoader:\n",
    "    \"\"\"Carica, pre-processa e suddivide il dataset GTZAN.\"\"\"\n",
    "    \n",
    "    def __init__(self, data_path, sample_rate=22050, n_mels=128, hop_length=512):\n",
    "        self.data_path = data_path\n",
    "        self.sample_rate = sample_rate\n",
    "        self.n_mels = n_mels\n",
    "        self.hop_length = hop_length\n",
    "        self.genres = sorted([d for d in os.listdir(data_path) if os.path.isdir(os.path.join(data_path, d))])\n",
    "        self.label_encoder = LabelEncoder().fit(self.genres)\n",
    "        self.scaler = StandardScaler()\n",
    "        \n",
    "    def load_file_paths(self):\n",
    "        file_paths, labels = [], []\n",
    "        for genre in self.genres:\n",
    "            genre_path = os.path.join(self.data_path, genre)\n",
    "            for filename in os.listdir(genre_path):\n",
    "                if filename.endswith(('.wav', '.au')):\n",
    "                    file_paths.append(os.path.join(genre_path, filename))\n",
    "                    labels.append(genre)\n",
    "        return file_paths, labels\n",
    "    \n",
    "    def process_file(self, file_path, n_segments=10, segment_duration=2.97):\n",
    "        \"\"\"Aumenta a 10 segmenti da ~3s per massimizzare la data augmentation.\"\"\"\n",
    "        try:\n",
    "            signal, _ = librosa.load(file_path, sr=self.sample_rate)\n",
    "            samples_per_segment = int(self.sample_rate * segment_duration)\n",
    "            \n",
    "            spectrograms = []\n",
    "            for s in range(n_segments):\n",
    "                start = s * samples_per_segment\n",
    "                end = start + samples_per_segment\n",
    "                if end <= len(signal):\n",
    "                    segment_signal = signal[start:end]\n",
    "                    mel_spec = librosa.feature.melspectrogram(\n",
    "                        y=segment_signal, sr=self.sample_rate, n_mels=self.n_mels, hop_length=self.hop_length)\n",
    "                    log_mel_spec = librosa.power_to_db(mel_spec, ref=np.max)\n",
    "                    spectrograms.append(log_mel_spec)\n",
    "            return spectrograms\n",
    "        except Exception as e:\n",
    "            print(f\"Attenzione: Errore nel processare {os.path.basename(file_path)}: {e}\")\n",
    "            return []\n",
    "            \n",
    "    def create_dataset_from_files(self, file_paths, labels_text, n_segments=10):\n",
    "        X_list, y_list = [], []\n",
    "        encoded_labels = self.label_encoder.transform(labels_text)\n",
    "        for i, file_path in enumerate(tqdm(file_paths, desc=f\"Processing {len(file_paths)} files\")):\n",
    "            spectrograms = self.process_file(file_path, n_segments=n_segments)\n",
    "            for spec in spectrograms:\n",
    "                X_list.append(spec)\n",
    "                y_list.append(encoded_labels[i])\n",
    "        return X_list, np.array(y_list)\n",
    "\n",
    "def adjust_spectrograms_shape(spec_list, target_len=128):\n",
    "    \"\"\"\n",
    "    CORREZIONE: Ripristinato il corpo della funzione per risolvere il NameError.\n",
    "    Uniforma gli spettrogrammi alla lunghezza target.\n",
    "    \"\"\"\n",
    "    print(f\"   - Uniformazione degli spettrogrammi alla lunghezza: {target_len} frame\")\n",
    "    adjusted_list = []\n",
    "    for spec in spec_list:\n",
    "        if spec.shape[1] > target_len:\n",
    "            adjusted_list.append(spec[:, :target_len])\n",
    "        else:\n",
    "            padding = target_len - spec.shape[1]\n",
    "            adjusted_list.append(np.pad(spec, ((0, 0), (0, padding)), mode='constant'))\n",
    "    return np.array(adjusted_list)\n",
    "\n",
    "print(\"âœ… Classe GTZANDataLoader definita e corretta.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d7b3cca6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "838637d88ede4ba0ad8606e07300d801",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing 600 files:   0%|          | 0/600 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_24226/2071647414.py:30: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  signal, _ = librosa.load(file_path, sr=self.sample_rate)\n",
      "/home/alepot55/Desktop/projects/naml_project/venv/lib/python3.12/site-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
      "\tDeprecated as of librosa version 0.10.0.\n",
      "\tIt will be removed in librosa version 1.0.\n",
      "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attenzione: Errore nel processare jazz.00054.wav: \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4888c3bfb2cf4323a4c6c649265f7f4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing 200 files:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2dc144d4a604be7a63c11e40c4a890f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing 200 files:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   - Uniformazione degli spettrogrammi alla lunghezza: 128 frame\n",
      "   - Uniformazione degli spettrogrammi alla lunghezza: 128 frame\n",
      "   - Uniformazione degli spettrogrammi alla lunghezza: 128 frame\n",
      "\n",
      "ðŸ’¾ Salvataggio dei dati processati e degli artefatti...\n",
      "\n",
      "âœ… Processo completato. I dati sono pronti per il training nel notebook '01_Model_Training'.\n",
      "\n",
      "ðŸ“Š Riepilogo Dati Salvati:\n",
      "   - Training Set:   X=(5990, 128, 128, 1), y=(5990,)\n",
      "   - Validation Set: X=(2000, 128, 128, 1), y=(2000,)\n",
      "   - Test Set:       X=(2000, 128, 128, 1), y=(2000,)\n"
     ]
    }
   ],
   "source": [
    "# ===================================================================\n",
    "# CELLA 3: ESECUZIONE E SALVATAGGIO\n",
    "# ===================================================================\n",
    "\n",
    "# 1. Caricamento percorsi e suddivisione\n",
    "data_loader = GTZANDataLoader(data_path=DATA_PATH)\n",
    "file_paths, labels_text = data_loader.load_file_paths()\n",
    "\n",
    "train_files, test_files, train_labels_text, test_labels_text = train_test_split(\n",
    "    file_paths, labels_text, test_size=0.2, random_state=RANDOM_STATE, stratify=labels_text)\n",
    "train_files, val_files, train_labels_text, val_labels_text = train_test_split(\n",
    "    train_files, train_labels_text, test_size=0.25, random_state=RANDOM_STATE, stratify=train_labels_text)\n",
    "# 2. Estrazione spettrogrammi\n",
    "X_train_list, y_train = data_loader.create_dataset_from_files(train_files, train_labels_text)\n",
    "X_val_list, y_val = data_loader.create_dataset_from_files(val_files, val_labels_text)\n",
    "X_test_list, y_test = data_loader.create_dataset_from_files(test_files, test_labels_text)\n",
    "\n",
    "# 3. Uniformazione shape\n",
    "X_train = adjust_spectrograms_shape(X_train_list)\n",
    "X_val = adjust_spectrograms_shape(X_val_list)\n",
    "X_test = adjust_spectrograms_shape(X_test_list)\n",
    "\n",
    "# 4. Normalizzazione\n",
    "scaler = data_loader.scaler\n",
    "X_train_shape = X_train.shape\n",
    "X_train = scaler.fit_transform(X_train.reshape(-1, X_train_shape[1] * X_train_shape[2])).reshape(X_train_shape)\n",
    "X_val = scaler.transform(X_val.reshape(-1, X_val.shape[1] * X_val.shape[2])).reshape(X_val.shape)\n",
    "X_test = scaler.transform(X_test.reshape(-1, X_test.shape[1] * X_test.shape[2])).reshape(X_test.shape)\n",
    "\n",
    "# 5. Aggiunta canale\n",
    "X_train, X_val, X_test = X_train[..., np.newaxis], X_val[..., np.newaxis], X_test[..., np.newaxis]\n",
    "\n",
    "# --- SALVATAGGIO DEGLI ARTEFATTI ---\n",
    "print(\"\\nðŸ’¾ Salvataggio dei dati processati e degli artefatti...\")\n",
    "np.save(os.path.join(PROCESSED_DATA_PATH, 'X_train.npy'), X_train)\n",
    "np.save(os.path.join(PROCESSED_DATA_PATH, 'y_train.npy'), y_train)\n",
    "np.save(os.path.join(PROCESSED_DATA_PATH, 'X_val.npy'), X_val)\n",
    "np.save(os.path.join(PROCESSED_DATA_PATH, 'y_val.npy'), y_val)\n",
    "np.save(os.path.join(PROCESSED_DATA_PATH, 'X_test.npy'), X_test)\n",
    "np.save(os.path.join(PROCESSED_DATA_PATH, 'y_test.npy'), y_test)\n",
    "\n",
    "with open(os.path.join(PROCESSED_DATA_PATH, 'scaler.pkl'), 'wb') as f:\n",
    "    pickle.dump(scaler, f)\n",
    "with open(os.path.join(PROCESSED_DATA_PATH, 'label_encoder.pkl'), 'wb') as f:\n",
    "    pickle.dump(data_loader.label_encoder, f)\n",
    "print(\"\\nâœ… Processo completato. I dati sono pronti per il training nel notebook '01_Model_Training'.\")\n",
    "print(\"\\nðŸ“Š Riepilogo Dati Salvati:\")\n",
    "print(f\"   - Training Set:   X={X_train.shape}, y={y_train.shape}\")\n",
    "print(f\"   - Validation Set: X={X_val.shape}, y={y_val.shape}\")\n",
    "print(f\"   - Test Set:       X={X_test.shape}, y={y_test.shape}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
