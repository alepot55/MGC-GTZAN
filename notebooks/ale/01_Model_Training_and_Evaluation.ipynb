{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dec7b1a4",
   "metadata": {},
   "source": [
    "# Notebook 01: Addestramento e Valutazione dei Modelli\n",
    "\n",
    "**Scopo:** Questo notebook carica i dati pre-processati dal Notebook 00, definisce le architetture delle reti neurali, orchestra un ciclo di esperimenti per addestrare e valutare diverse combinazioni di modelli e ottimizzatori, e salva gli artefatti migliori per l'analisi successiva.\n",
    "\n",
    "**Input:**\n",
    "- Dati pre-processati da `../data/processed/` (`X_train.npy`, `y_train.npy`, etc.)\n",
    "\n",
    "**Output (salvati in `../models/` e `../reports/`):**\n",
    "- I modelli migliori per ogni esperimento (es. `UNet_Lite_Adam.keras`).\n",
    "- Un file di riepilogo con le metriche di performance (es. `training_summary.csv`).\n",
    "- (Opzionale) Le storie di training salvate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7fe0892b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ GPU(s) Trovata/e: ['NVIDIA GeForce RTX 4070']\n",
      "‚úÖ Politica di Mixed Precision impostata su: mixed_float16\n",
      "\n",
      "üîÑ Caricamento dei dati pre-processati...\n",
      "\n",
      "‚úÖ Dati caricati con successo.\n",
      "   - Shape X_train: (5990, 128, 128, 1) | Shape y_train_cat: (5990, 10)\n",
      "   - Numero di classi: 10\n"
     ]
    }
   ],
   "source": [
    "# ===================================================================\n",
    "# CELLA 1: SETUP, IMPORTS E CARICAMENTO DATI\n",
    "# ===================================================================\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "import traceback\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras as keras\n",
    "from keras import layers, models, optimizers, callbacks, regularizers\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "# --- Configurazione Globale ---\n",
    "PROCESSED_DATA_PATH = '../../data/processed/'\n",
    "MODELS_PATH = '../../models/ale/'\n",
    "REPORTS_PATH = '../../reports/'\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "os.makedirs(MODELS_PATH, exist_ok=True)\n",
    "os.makedirs(REPORTS_PATH, exist_ok=True)\n",
    "\n",
    "# 1. GPU e Mixed Precision\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus: tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        print(f\"‚úÖ GPU(s) Trovata/e: {[tf.config.experimental.get_device_details(g)['device_name'] for g in gpus]}\")\n",
    "        policy = keras.mixed_precision.Policy('mixed_float16')\n",
    "        keras.mixed_precision.set_global_policy(policy)\n",
    "        print(f\"‚úÖ Politica di Mixed Precision impostata su: {keras.mixed_precision.global_policy().name}\")\n",
    "    except RuntimeError as e: print(f\"‚ö†Ô∏è Errore durante l'inizializzazione della GPU: {e}\")\n",
    "else: print(\"‚ùå NESSUNA GPU TROVATA. L'allenamento sar√† su CPU.\")\n",
    "\n",
    "# 2. Caricamento Dati Pre-processati\n",
    "print(\"\\nüîÑ Caricamento dei dati pre-processati...\")\n",
    "try:\n",
    "    X_train = np.load(os.path.join(PROCESSED_DATA_PATH, 'X_train.npy'))\n",
    "    y_train = np.load(os.path.join(PROCESSED_DATA_PATH, 'y_train.npy'))\n",
    "    X_val = np.load(os.path.join(PROCESSED_DATA_PATH, 'X_val.npy'))\n",
    "    y_val = np.load(os.path.join(PROCESSED_DATA_PATH, 'y_val.npy'))\n",
    "    X_test = np.load(os.path.join(PROCESSED_DATA_PATH, 'X_test.npy'))\n",
    "    y_test = np.load(os.path.join(PROCESSED_DATA_PATH, 'y_test.npy'))\n",
    "    \n",
    "    with open(os.path.join(PROCESSED_DATA_PATH, 'label_encoder.pkl'), 'rb') as f:\n",
    "        label_encoder = pickle.load(f)\n",
    "\n",
    "    # Conversione in formato categorico\n",
    "    num_classes = len(label_encoder.classes_)\n",
    "    y_train_cat = to_categorical(y_train, num_classes=num_classes)\n",
    "    y_val_cat = to_categorical(y_val, num_classes=num_classes)\n",
    "    y_test_cat = to_categorical(y_test, num_classes=num_classes)\n",
    "    \n",
    "    print(\"\\n‚úÖ Dati caricati con successo.\")\n",
    "    print(f\"   - Shape X_train: {X_train.shape} | Shape y_train_cat: {y_train_cat.shape}\")\n",
    "    print(f\"   - Numero di classi: {num_classes}\")\n",
    "except FileNotFoundError:\n",
    "    print(\"‚ùå ERRORE: File di dati non trovati. Eseguire prima il notebook '00_Setup_and_Data_Preparation.ipynb'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56958c89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ ModelFactory defined with 5 candidate architectures for the final comparative analysis.\n"
     ]
    }
   ],
   "source": [
    "# ===================================================================\n",
    "# CELL 2: COMPREHENSIVE MODEL FACTORY FOR COMPARATIVE ANALYSIS\n",
    "# ===================================================================\n",
    "# This cell defines all candidate architectures for our final analysis.\n",
    "# The selection progresses from simple and efficient baselines to more\n",
    "# complex residual networks, allowing for a thorough evaluation of\n",
    "# architectural trade-offs. All code is written in English for clarity.\n",
    "\n",
    "from keras import layers, models, regularizers\n",
    "\n",
    "class ModelFactory:\n",
    "    \"\"\"\n",
    "    A comprehensive factory for building and comparing a curated set of CNN architectures.\n",
    "    This class centralizes our key models for the final comparative experiment.\n",
    "    \"\"\"\n",
    "    \n",
    "    # -------------------------------------------------------------------\n",
    "    # Helper Building Blocks (Shared across multiple architectures)\n",
    "    # -------------------------------------------------------------------\n",
    "\n",
    "    @staticmethod\n",
    "    def _se_block(input_tensor, ratio=8):\n",
    "        \"\"\"\n",
    "        Squeeze-and-Excitation block. A lightweight channel-wise attention\n",
    "        mechanism to recalibrate feature maps by modeling interdependencies\n",
    "        between channels.\n",
    "        Ref: Hu et al., \"Squeeze-and-Excitation Networks\" (2018)\n",
    "        \"\"\"\n",
    "        channels = input_tensor.shape[-1]\n",
    "        # Squeeze: Global information embedding\n",
    "        se = layers.GlobalAveragePooling2D(name=f'se_squeeze_{input_tensor.name}')(input_tensor)\n",
    "        se = layers.Reshape((1, 1, channels))(se)\n",
    "        # Excitation: Adaptive recalibration\n",
    "        se = layers.Dense(channels // ratio, activation='relu', name=f'se_excite_1_{input_tensor.name}')(se)\n",
    "        se = layers.Dense(channels, activation='sigmoid', name=f'se_excite_2_{input_tensor.name}')(se)\n",
    "        return layers.Multiply(name=f'se_scale_{input_tensor.name}')([input_tensor, se])\n",
    "\n",
    "    # -------------------------------------------------------------------\n",
    "    # Model 1: Efficient VGG-style Baseline\n",
    "    # -------------------------------------------------------------------\n",
    "    @staticmethod\n",
    "    def build_efficient_vgg(input_shape, num_classes):\n",
    "        \"\"\"\n",
    "        A memory-efficient VGG-style model. Starts with a small number of\n",
    "        filters to establish a fast, simple, and memory-safe baseline.\n",
    "        \"\"\"\n",
    "        inputs = layers.Input(shape=input_shape)\n",
    "        x = layers.Conv2D(16, 3, padding='same', use_bias=False)(inputs)\n",
    "        x = layers.BatchNormalization()(x); x = layers.Activation('relu')(x)\n",
    "        x = layers.MaxPooling2D(2)(x); x = ModelFactory._se_block(x)\n",
    "        x = layers.Conv2D(32, 3, padding='same', use_bias=False)(x)\n",
    "        x = layers.BatchNormalization()(x); x = layers.Activation('relu')(x)\n",
    "        x = layers.MaxPooling2D(2)(x); x = ModelFactory._se_block(x)\n",
    "        x = layers.Conv2D(64, 3, padding='same', use_bias=False)(x)\n",
    "        x = layers.BatchNormalization()(x); x = layers.Activation('relu')(x)\n",
    "        x = layers.MaxPooling2D(2)(x); x = ModelFactory._se_block(x)\n",
    "        x = layers.GlobalAveragePooling2D()(x)\n",
    "        x = layers.Dense(128, activation='relu')(x)\n",
    "        x = layers.Dropout(0.5)(x)\n",
    "        outputs = layers.Dense(num_classes, activation='softmax', dtype='float32')(x)\n",
    "        return models.Model(inputs=inputs, outputs=outputs, name='Efficient_VGG')\n",
    "\n",
    "    # -------------------------------------------------------------------\n",
    "    # Model 2: Paper-Inspired Multi-Scale Model\n",
    "    # -------------------------------------------------------------------\n",
    "    @staticmethod\n",
    "    def build_paper_cnn_lite(input_shape, num_classes):\n",
    "        \"\"\"\n",
    "        A memory-optimized interpretation of the paper's multi-scale feature\n",
    "        aggregation concept, using PReLU activation as specified.\n",
    "        \"\"\"\n",
    "        inputs = layers.Input(shape=input_shape)\n",
    "        x1 = layers.Conv2D(16, 3, padding='same', use_bias=False)(inputs)\n",
    "        x1 = layers.BatchNormalization()(x1); x1 = layers.PReLU(shared_axes=[1, 2])(x1)\n",
    "        p1 = layers.MaxPooling2D(2)(x1)\n",
    "        x2 = layers.Conv2D(32, 3, padding='same', use_bias=False)(p1)\n",
    "        x2 = layers.BatchNormalization()(x2); x2 = layers.PReLU(shared_axes=[1, 2])(x2)\n",
    "        p2 = layers.MaxPooling2D(2)(x2)\n",
    "        x3 = layers.Conv2D(64, 3, padding='same', use_bias=False)(p2)\n",
    "        x3 = layers.BatchNormalization()(x3); x3 = layers.PReLU(shared_axes=[1, 2])(x3)\n",
    "        gap1 = layers.GlobalAveragePooling2D()(x1)\n",
    "        gap2 = layers.GlobalAveragePooling2D()(x2)\n",
    "        gap3 = layers.GlobalAveragePooling2D()(x3)\n",
    "        merged_features = layers.Concatenate()([gap1, gap2, gap3])\n",
    "        x = layers.Dense(128, activation='relu')(merged_features)\n",
    "        x = layers.Dropout(0.5)(x)\n",
    "        outputs = layers.Dense(num_classes, activation='softmax', dtype='float32')(x)\n",
    "        return models.Model(inputs=inputs, outputs=outputs, name='PaperCNN_Lite')\n",
    "\n",
    "    # -------------------------------------------------------------------\n",
    "    # Model 3: VGG with Attention\n",
    "    # -------------------------------------------------------------------\n",
    "    @staticmethod\n",
    "    def build_se_audio_cnn(input_shape, num_classes):\n",
    "        \"\"\"\n",
    "        A standard VGG-style architecture enhanced with SE blocks. Tests\n",
    "        the impact of attention on a conventional, non-residual backbone.\n",
    "        \"\"\"\n",
    "        inputs = layers.Input(shape=input_shape)\n",
    "        x = layers.Conv2D(32, 3, padding='same', use_bias=False)(inputs)\n",
    "        x = layers.BatchNormalization()(x); x = layers.Activation('relu')(x)\n",
    "        x = layers.MaxPooling2D(2)(x); x = ModelFactory._se_block(x)\n",
    "        x = layers.Conv2D(64, 3, padding='same', use_bias=False)(x)\n",
    "        x = layers.BatchNormalization()(x); x = layers.Activation('relu')(x)\n",
    "        x = layers.MaxPooling2D(2)(x); x = ModelFactory._se_block(x)\n",
    "        x = layers.Conv2D(128, 3, padding='same', use_bias=False)(x)\n",
    "        x = layers.BatchNormalization()(x); x = layers.Activation('relu')(x)\n",
    "        x = layers.MaxPooling2D(2)(x); x = ModelFactory._se_block(x)\n",
    "        x = layers.GlobalAveragePooling2D()(x)\n",
    "        x = layers.Dense(128, activation='relu')(x)\n",
    "        x = layers.Dropout(0.5)(x)\n",
    "        outputs = layers.Dense(num_classes, activation='softmax', dtype='float32')(x)\n",
    "        return models.Model(inputs=inputs, outputs=outputs, name='SE_AudioCNN')\n",
    "\n",
    "    # -------------------------------------------------------------------\n",
    "    # Model 4: Residual Network with Separable Convolutions\n",
    "    # -------------------------------------------------------------------\n",
    "    @staticmethod\n",
    "    def _separable_res_se_block(input_tensor, filters, stride=1):\n",
    "        \"\"\"Residual block using depthwise separable convolutions for efficiency.\"\"\"\n",
    "        shortcut = input_tensor\n",
    "        x = layers.SeparableConv2D(filters, 3, strides=stride, padding='same', use_bias=False)(input_tensor)\n",
    "        x = layers.BatchNormalization()(x); x = layers.PReLU(shared_axes=[1, 2])(x)\n",
    "        x = layers.SeparableConv2D(filters, 3, padding='same', use_bias=False)(x)\n",
    "        x = layers.BatchNormalization()(x); x = ModelFactory._se_block(x)\n",
    "        if stride > 1 or shortcut.shape[-1] != filters:\n",
    "            shortcut = layers.Conv2D(filters, 1, strides=stride, use_bias=False)(shortcut)\n",
    "            shortcut = layers.BatchNormalization()(shortcut)\n",
    "        x = layers.Add()([shortcut, x]); x = layers.PReLU(shared_axes=[1, 2])(x)\n",
    "        return x\n",
    "\n",
    "    @staticmethod\n",
    "    def build_separable_res_se_cnn(input_shape, num_classes):\n",
    "        \"\"\"A parametrically efficient ResNet-style model.\"\"\"\n",
    "        inputs = layers.Input(shape=input_shape)\n",
    "        x = layers.SeparableConv2D(32, 3, strides=1, padding='same', use_bias=False)(inputs)\n",
    "        x = layers.BatchNormalization()(x); x = layers.PReLU(shared_axes=[1, 2])(x)\n",
    "        x = ModelFactory._separable_res_se_block(x, 64, stride=2)\n",
    "        x = ModelFactory._separable_res_se_block(x, 128, stride=2)\n",
    "        x = ModelFactory._separable_res_se_block(x, 256, stride=2)\n",
    "        x = layers.GlobalAveragePooling2D()(x)\n",
    "        x = layers.Dropout(0.5)(x)\n",
    "        outputs = layers.Dense(num_classes, activation='softmax', dtype='float32')(x)\n",
    "        return models.Model(inputs=inputs, outputs=outputs, name='SeparableResSE_CNN')\n",
    "\n",
    "    # -------------------------------------------------------------------\n",
    "    # Model 5: Residual Network with Standard Convolutions\n",
    "    # -------------------------------------------------------------------\n",
    "    @staticmethod\n",
    "    def _res_se_block(input_tensor, filters, stride=1):\n",
    "        \"\"\"Residual block using standard convolutions.\"\"\"\n",
    "        shortcut = input_tensor\n",
    "        x = layers.Conv2D(filters, 3, strides=stride, padding='same', use_bias=False)(input_tensor)\n",
    "        x = layers.BatchNormalization()(x); x = layers.PReLU(shared_axes=[1, 2])(x)\n",
    "        x = layers.Conv2D(filters, 3, padding='same', use_bias=False)(x)\n",
    "        x = layers.BatchNormalization()(x); x = ModelFactory._se_block(x)\n",
    "        if stride > 1 or shortcut.shape[-1] != filters:\n",
    "            shortcut = layers.Conv2D(filters, 1, strides=stride, use_bias=False)(shortcut)\n",
    "            shortcut = layers.BatchNormalization()(shortcut)\n",
    "        x = layers.Add()([shortcut, x]); x = layers.PReLU(shared_axes=[1, 2])(x)\n",
    "        return x\n",
    "\n",
    "    @staticmethod\n",
    "    def build_res_se_audio_cnn(input_shape, num_classes):\n",
    "        \"\"\"\n",
    "        Our most powerful stable architecture, combining ResNet principles\n",
    "        with SE attention and standard convolutions.\n",
    "        \"\"\"\n",
    "        inputs = layers.Input(shape=input_shape)\n",
    "        x = layers.Conv2D(32, 3, strides=1, padding='same', use_bias=False)(inputs)\n",
    "        x = layers.BatchNormalization()(x); x = layers.PReLU(shared_axes=[1, 2])(x)\n",
    "        x = ModelFactory._res_se_block(x, 64, stride=2)\n",
    "        x = ModelFactory._res_se_block(x, 128, stride=2)\n",
    "        x = ModelFactory._res_se_block(x, 256, stride=2)\n",
    "        x = layers.GlobalAveragePooling2D()(x)\n",
    "        x = layers.Dropout(0.5)(x)\n",
    "        outputs = layers.Dense(num_classes, activation='softmax', dtype='float32')(x)\n",
    "        return models.Model(inputs=inputs, outputs=outputs, name='ResSE_AudioCNN')\n",
    "\n",
    "print(\"‚úÖ ModelFactory defined with 5 candidate architectures for the final comparative analysis.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5a6c0fb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pre-processed data...\n",
      "‚úÖ Data successfully loaded and prepared.\n",
      "\n",
      "Configuring JIT data pipelines (without caching)...\n",
      "\n",
      "================================================================================\n",
      "TRAINING ARCHITECTURE: 'Efficient_VGG'\n",
      "================================================================================\n",
      "üöÄ Starting training for model: Efficient_VGG...\n",
      "Epoch 01/50 | Time: 1753515507.21s | Loss: 1.9346 | Acc: 0.2965 | Val Loss: 2.2386 | Val Acc: 0.1785 | LR: 1.0e-03 ‚úÖ\n",
      "Epoch 02/50 | Time: 1753515508.27s | Loss: 1.5980 | Acc: 0.4020 | Val Loss: 2.2621 | Val Acc: 0.1930 | LR: 1.0e-03 ‚úÖ\n",
      "Epoch 03/50 | Time: 1753515509.31s | Loss: 1.4283 | Acc: 0.4910 | Val Loss: 2.6916 | Val Acc: 0.1785 | LR: 1.0e-03\n",
      "Epoch 04/50 | Time: 1753515510.31s | Loss: 1.3383 | Acc: 0.5279 | Val Loss: 2.0470 | Val Acc: 0.2690 | LR: 1.0e-03 ‚úÖ\n",
      "Epoch 05/50 | Time: 1753515511.36s | Loss: 1.2341 | Acc: 0.5644 | Val Loss: 1.9110 | Val Acc: 0.3975 | LR: 1.0e-03 ‚úÖ\n",
      "Epoch 06/50 | Time: 1753515512.46s | Loss: 1.2089 | Acc: 0.5669 | Val Loss: 1.4708 | Val Acc: 0.4855 | LR: 1.0e-03 ‚úÖ\n",
      "Epoch 07/50 | Time: 1753515513.54s | Loss: 1.1083 | Acc: 0.6000 | Val Loss: 1.3865 | Val Acc: 0.5005 | LR: 1.0e-03 ‚úÖ\n",
      "Epoch 08/50 | Time: 1753515514.60s | Loss: 1.0845 | Acc: 0.6232 | Val Loss: 1.0953 | Val Acc: 0.6190 | LR: 1.0e-03 ‚úÖ\n",
      "Epoch 09/50 | Time: 1753515515.65s | Loss: 1.0439 | Acc: 0.6401 | Val Loss: 1.4653 | Val Acc: 0.5075 | LR: 1.0e-03\n",
      "Epoch 10/50 | Time: 1753515516.65s | Loss: 0.9983 | Acc: 0.6503 | Val Loss: 1.2198 | Val Acc: 0.5740 | LR: 1.0e-03\n",
      "Epoch 11/50 | Time: 1753515517.64s | Loss: 0.9803 | Acc: 0.6618 | Val Loss: 1.1301 | Val Acc: 0.6160 | LR: 1.0e-03\n",
      "Epoch 12/50 | Time: 1753515518.63s | Loss: 0.9360 | Acc: 0.6723 | Val Loss: 1.2217 | Val Acc: 0.5965 | LR: 1.0e-03\n",
      "Epoch 13/50 | Time: 1753515519.63s | Loss: 0.9090 | Acc: 0.6853 | Val Loss: 1.3988 | Val Acc: 0.5455 | LR: 1.0e-03\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "Epoch 14/50 | Time: 1753515520.62s | Loss: 0.8537 | Acc: 0.7098 | Val Loss: 0.9374 | Val Acc: 0.6775 | LR: 2.0e-04 ‚úÖ\n",
      "Epoch 15/50 | Time: 1753515521.68s | Loss: 0.8092 | Acc: 0.7264 | Val Loss: 1.0524 | Val Acc: 0.6380 | LR: 2.0e-04\n",
      "Epoch 16/50 | Time: 1753515522.68s | Loss: 0.7866 | Acc: 0.7331 | Val Loss: 1.0168 | Val Acc: 0.6600 | LR: 2.0e-04\n",
      "Epoch 17/50 | Time: 1753515523.67s | Loss: 0.7836 | Acc: 0.7359 | Val Loss: 0.9326 | Val Acc: 0.6880 | LR: 2.0e-04 ‚úÖ\n",
      "Epoch 18/50 | Time: 1753515524.71s | Loss: 0.7852 | Acc: 0.7297 | Val Loss: 1.0393 | Val Acc: 0.6510 | LR: 2.0e-04\n",
      "Epoch 19/50 | Time: 1753515525.69s | Loss: 0.7745 | Acc: 0.7361 | Val Loss: 0.9860 | Val Acc: 0.6660 | LR: 2.0e-04\n",
      "Epoch 20/50 | Time: 1753515526.68s | Loss: 0.7837 | Acc: 0.7324 | Val Loss: 1.0106 | Val Acc: 0.6600 | LR: 2.0e-04\n",
      "Epoch 21/50 | Time: 1753515527.69s | Loss: 0.7746 | Acc: 0.7376 | Val Loss: 0.9798 | Val Acc: 0.6625 | LR: 2.0e-04\n",
      "Epoch 22/50 | Time: 1753515528.69s | Loss: 0.7584 | Acc: 0.7424 | Val Loss: 1.0384 | Val Acc: 0.6605 | LR: 2.0e-04\n",
      "\n",
      "Epoch 22: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
      "Epoch 23/50 | Time: 1753515529.68s | Loss: 0.7428 | Acc: 0.7521 | Val Loss: 0.8773 | Val Acc: 0.7125 | LR: 4.0e-05 ‚úÖ\n",
      "Epoch 24/50 | Time: 1753515530.74s | Loss: 0.7409 | Acc: 0.7514 | Val Loss: 0.8861 | Val Acc: 0.7000 | LR: 4.0e-05\n",
      "Epoch 25/50 | Time: 1753515531.74s | Loss: 0.7379 | Acc: 0.7481 | Val Loss: 0.9002 | Val Acc: 0.6960 | LR: 4.0e-05\n",
      "Epoch 26/50 | Time: 1753515532.75s | Loss: 0.7321 | Acc: 0.7518 | Val Loss: 0.8735 | Val Acc: 0.7020 | LR: 4.0e-05\n",
      "Epoch 27/50 | Time: 1753515533.76s | Loss: 0.7405 | Acc: 0.7446 | Val Loss: 0.8882 | Val Acc: 0.7120 | LR: 4.0e-05\n",
      "Epoch 28/50 | Time: 1753515534.77s | Loss: 0.7213 | Acc: 0.7624 | Val Loss: 0.9538 | Val Acc: 0.6870 | LR: 4.0e-05\n",
      "Epoch 29/50 | Time: 1753515535.76s | Loss: 0.7457 | Acc: 0.7451 | Val Loss: 0.8794 | Val Acc: 0.7025 | LR: 4.0e-05\n",
      "Epoch 30/50 | Time: 1753515536.76s | Loss: 0.7331 | Acc: 0.7504 | Val Loss: 0.8733 | Val Acc: 0.7050 | LR: 4.0e-05\n",
      "Epoch 31/50 | Time: 1753515537.77s | Loss: 0.7209 | Acc: 0.7556 | Val Loss: 0.9035 | Val Acc: 0.6935 | LR: 4.0e-05\n",
      "Epoch 32/50 | Time: 1753515538.76s | Loss: 0.7222 | Acc: 0.7624 | Val Loss: 0.8869 | Val Acc: 0.7035 | LR: 4.0e-05\n",
      "Epoch 33/50 | Time: 1753515539.77s | Loss: 0.7320 | Acc: 0.7538 | Val Loss: 0.8811 | Val Acc: 0.7090 | LR: 4.0e-05\n",
      "Epoch 34/50 | Time: 1753515540.77s | Loss: 0.7309 | Acc: 0.7509 | Val Loss: 0.8855 | Val Acc: 0.7035 | LR: 4.0e-05\n",
      "Epoch 35/50 | Time: 1753515541.78s | Loss: 0.7284 | Acc: 0.7604 | Val Loss: 0.9089 | Val Acc: 0.7010 | LR: 4.0e-05\n",
      "\n",
      "Epoch 35: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.\n",
      "Epoch 36/50 | Time: 1753515542.83s | Loss: 0.7117 | Acc: 0.7619 | Val Loss: 0.8709 | Val Acc: 0.7120 | LR: 8.0e-06\n",
      "Epoch 37/50 | Time: 1753515543.85s | Loss: 0.7284 | Acc: 0.7618 | Val Loss: 0.8632 | Val Acc: 0.7105 | LR: 8.0e-06\n",
      "Epoch 38/50 | Time: 1753515544.87s | Loss: 0.7139 | Acc: 0.7574 | Val Loss: 0.8649 | Val Acc: 0.7125 | LR: 8.0e-06\n",
      "üèÅ Finished training. Best Validation Accuracy: 0.7125\n",
      "\n",
      "================================================================================\n",
      "TRAINING ARCHITECTURE: 'PaperCNN_Lite'\n",
      "================================================================================\n",
      "üöÄ Starting training for model: PaperCNN_Lite...\n",
      "Epoch 01/50 | Time: 1753515553.35s | Loss: 1.9053 | Acc: 0.3042 | Val Loss: 2.1395 | Val Acc: 0.1885 | LR: 1.0e-03 ‚úÖ\n",
      "Epoch 02/50 | Time: 1753515554.58s | Loss: 1.6026 | Acc: 0.4147 | Val Loss: 2.2796 | Val Acc: 0.2120 | LR: 1.0e-03 ‚úÖ\n",
      "Epoch 03/50 | Time: 1753515555.68s | Loss: 1.4939 | Acc: 0.4554 | Val Loss: 2.3849 | Val Acc: 0.2290 | LR: 1.0e-03 ‚úÖ\n",
      "Epoch 04/50 | Time: 1753515556.84s | Loss: 1.4042 | Acc: 0.4878 | Val Loss: 2.1345 | Val Acc: 0.2995 | LR: 1.0e-03 ‚úÖ\n",
      "Epoch 05/50 | Time: 1753515557.99s | Loss: 1.3192 | Acc: 0.5324 | Val Loss: 2.2053 | Val Acc: 0.2795 | LR: 1.0e-03\n",
      "Epoch 06/50 | Time: 1753515559.08s | Loss: 1.2194 | Acc: 0.5751 | Val Loss: 1.8423 | Val Acc: 0.4030 | LR: 1.0e-03 ‚úÖ\n",
      "Epoch 07/50 | Time: 1753515560.20s | Loss: 1.1729 | Acc: 0.5938 | Val Loss: 1.4821 | Val Acc: 0.4925 | LR: 1.0e-03 ‚úÖ\n",
      "Epoch 08/50 | Time: 1753515561.32s | Loss: 1.1399 | Acc: 0.5973 | Val Loss: 1.3834 | Val Acc: 0.5170 | LR: 1.0e-03 ‚úÖ\n",
      "Epoch 09/50 | Time: 1753515562.45s | Loss: 1.1104 | Acc: 0.6172 | Val Loss: 2.9594 | Val Acc: 0.3660 | LR: 1.0e-03\n",
      "Epoch 10/50 | Time: 1753515563.54s | Loss: 1.0692 | Acc: 0.6324 | Val Loss: 1.7042 | Val Acc: 0.4800 | LR: 1.0e-03\n",
      "Epoch 11/50 | Time: 1753515564.63s | Loss: 1.0545 | Acc: 0.6374 | Val Loss: 1.3313 | Val Acc: 0.5715 | LR: 1.0e-03 ‚úÖ\n",
      "Epoch 12/50 | Time: 1753515565.75s | Loss: 1.0252 | Acc: 0.6461 | Val Loss: 1.2547 | Val Acc: 0.5840 | LR: 1.0e-03 ‚úÖ\n",
      "Epoch 13/50 | Time: 1753515566.87s | Loss: 0.9947 | Acc: 0.6589 | Val Loss: 1.2285 | Val Acc: 0.5910 | LR: 1.0e-03 ‚úÖ\n",
      "Epoch 14/50 | Time: 1753515568.00s | Loss: 0.9726 | Acc: 0.6656 | Val Loss: 2.0446 | Val Acc: 0.4305 | LR: 1.0e-03\n",
      "Epoch 15/50 | Time: 1753515569.15s | Loss: 0.9412 | Acc: 0.6768 | Val Loss: 1.2560 | Val Acc: 0.5950 | LR: 1.0e-03 ‚úÖ\n",
      "Epoch 16/50 | Time: 1753515570.34s | Loss: 0.9098 | Acc: 0.6891 | Val Loss: 1.5784 | Val Acc: 0.5070 | LR: 1.0e-03\n",
      "Epoch 17/50 | Time: 1753515571.52s | Loss: 0.9057 | Acc: 0.6937 | Val Loss: 1.0709 | Val Acc: 0.6350 | LR: 1.0e-03 ‚úÖ\n",
      "Epoch 18/50 | Time: 1753515572.68s | Loss: 0.8900 | Acc: 0.7012 | Val Loss: 1.7870 | Val Acc: 0.5085 | LR: 1.0e-03\n",
      "Epoch 19/50 | Time: 1753515573.86s | Loss: 0.8769 | Acc: 0.6998 | Val Loss: 3.0441 | Val Acc: 0.3645 | LR: 1.0e-03\n",
      "Epoch 20/50 | Time: 1753515575.00s | Loss: 0.8831 | Acc: 0.6973 | Val Loss: 1.4774 | Val Acc: 0.5705 | LR: 1.0e-03\n",
      "Epoch 21/50 | Time: 1753515576.15s | Loss: 0.8379 | Acc: 0.7147 | Val Loss: 2.3021 | Val Acc: 0.4010 | LR: 1.0e-03\n",
      "Epoch 22/50 | Time: 1753515577.31s | Loss: 0.8416 | Acc: 0.7150 | Val Loss: 3.0340 | Val Acc: 0.3630 | LR: 1.0e-03\n",
      "\n",
      "Epoch 22: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "Epoch 23/50 | Time: 1753515578.42s | Loss: 0.7675 | Acc: 0.7439 | Val Loss: 0.9356 | Val Acc: 0.7035 | LR: 2.0e-04 ‚úÖ\n",
      "Epoch 24/50 | Time: 1753515579.63s | Loss: 0.7443 | Acc: 0.7482 | Val Loss: 0.9994 | Val Acc: 0.6805 | LR: 2.0e-04\n",
      "Epoch 25/50 | Time: 1753515580.71s | Loss: 0.7599 | Acc: 0.7419 | Val Loss: 1.0551 | Val Acc: 0.6600 | LR: 2.0e-04\n",
      "Epoch 26/50 | Time: 1753515581.80s | Loss: 0.7344 | Acc: 0.7563 | Val Loss: 1.1264 | Val Acc: 0.6465 | LR: 2.0e-04\n",
      "Epoch 27/50 | Time: 1753515582.88s | Loss: 0.7360 | Acc: 0.7511 | Val Loss: 1.0166 | Val Acc: 0.6715 | LR: 2.0e-04\n",
      "Epoch 28/50 | Time: 1753515583.97s | Loss: 0.7384 | Acc: 0.7504 | Val Loss: 1.0163 | Val Acc: 0.6805 | LR: 2.0e-04\n",
      "\n",
      "Epoch 28: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
      "Epoch 29/50 | Time: 1753515585.08s | Loss: 0.7110 | Acc: 0.7594 | Val Loss: 0.9249 | Val Acc: 0.7030 | LR: 4.0e-05\n",
      "Epoch 30/50 | Time: 1753515586.17s | Loss: 0.7197 | Acc: 0.7604 | Val Loss: 0.9054 | Val Acc: 0.7140 | LR: 4.0e-05 ‚úÖ\n",
      "Epoch 31/50 | Time: 1753515587.30s | Loss: 0.7095 | Acc: 0.7551 | Val Loss: 0.9175 | Val Acc: 0.7025 | LR: 4.0e-05\n",
      "Epoch 32/50 | Time: 1753515588.38s | Loss: 0.7084 | Acc: 0.7619 | Val Loss: 0.8986 | Val Acc: 0.7115 | LR: 4.0e-05\n",
      "Epoch 33/50 | Time: 1753515589.47s | Loss: 0.7044 | Acc: 0.7653 | Val Loss: 0.9179 | Val Acc: 0.7115 | LR: 4.0e-05\n",
      "Epoch 34/50 | Time: 1753515590.56s | Loss: 0.7029 | Acc: 0.7619 | Val Loss: 0.9000 | Val Acc: 0.7160 | LR: 4.0e-05 ‚úÖ\n",
      "Epoch 35/50 | Time: 1753515591.70s | Loss: 0.6884 | Acc: 0.7664 | Val Loss: 0.9080 | Val Acc: 0.7130 | LR: 4.0e-05\n",
      "Epoch 36/50 | Time: 1753515592.87s | Loss: 0.7137 | Acc: 0.7593 | Val Loss: 0.9394 | Val Acc: 0.7050 | LR: 4.0e-05\n",
      "Epoch 37/50 | Time: 1753515594.01s | Loss: 0.7034 | Acc: 0.7669 | Val Loss: 0.8992 | Val Acc: 0.7205 | LR: 4.0e-05 ‚úÖ\n",
      "\n",
      "Epoch 37: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.\n",
      "Epoch 38/50 | Time: 1753515595.24s | Loss: 0.7001 | Acc: 0.7654 | Val Loss: 0.8876 | Val Acc: 0.7230 | LR: 8.0e-06 ‚úÖ\n",
      "Epoch 39/50 | Time: 1753515596.45s | Loss: 0.6980 | Acc: 0.7638 | Val Loss: 0.8826 | Val Acc: 0.7235 | LR: 8.0e-06 ‚úÖ\n",
      "Epoch 40/50 | Time: 1753515597.62s | Loss: 0.7093 | Acc: 0.7651 | Val Loss: 0.8863 | Val Acc: 0.7215 | LR: 8.0e-06\n",
      "Epoch 41/50 | Time: 1753515598.79s | Loss: 0.6942 | Acc: 0.7708 | Val Loss: 0.8840 | Val Acc: 0.7230 | LR: 8.0e-06\n",
      "Epoch 42/50 | Time: 1753515599.92s | Loss: 0.6944 | Acc: 0.7656 | Val Loss: 0.8876 | Val Acc: 0.7215 | LR: 8.0e-06\n",
      "Epoch 43/50 | Time: 1753515601.07s | Loss: 0.7045 | Acc: 0.7656 | Val Loss: 0.8846 | Val Acc: 0.7220 | LR: 8.0e-06\n",
      "Epoch 44/50 | Time: 1753515602.21s | Loss: 0.7056 | Acc: 0.7634 | Val Loss: 0.8817 | Val Acc: 0.7240 | LR: 8.0e-06 ‚úÖ\n",
      "Epoch 45/50 | Time: 1753515603.40s | Loss: 0.7159 | Acc: 0.7611 | Val Loss: 0.8834 | Val Acc: 0.7215 | LR: 8.0e-06\n",
      "Epoch 46/50 | Time: 1753515604.58s | Loss: 0.6932 | Acc: 0.7674 | Val Loss: 0.8826 | Val Acc: 0.7225 | LR: 8.0e-06\n",
      "Epoch 47/50 | Time: 1753515605.69s | Loss: 0.6970 | Acc: 0.7681 | Val Loss: 0.8837 | Val Acc: 0.7275 | LR: 8.0e-06 ‚úÖ\n",
      "Epoch 48/50 | Time: 1753515606.92s | Loss: 0.6978 | Acc: 0.7669 | Val Loss: 0.8850 | Val Acc: 0.7255 | LR: 8.0e-06\n",
      "Epoch 49/50 | Time: 1753515608.05s | Loss: 0.6939 | Acc: 0.7656 | Val Loss: 0.8867 | Val Acc: 0.7235 | LR: 8.0e-06\n",
      "\n",
      "Epoch 49: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.\n",
      "Epoch 50/50 | Time: 1753515609.19s | Loss: 0.6973 | Acc: 0.7676 | Val Loss: 0.8829 | Val Acc: 0.7245 | LR: 1.6e-06\n",
      "üèÅ Finished training. Best Validation Accuracy: 0.7275\n",
      "\n",
      "================================================================================\n",
      "TRAINING ARCHITECTURE: 'SE_AudioCNN'\n",
      "================================================================================\n",
      "üöÄ Starting training for model: SE_AudioCNN...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-26 09:40:15.395220: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_2207', 16 bytes spill stores, 16 bytes spill loads\n",
      "\n",
      "2025-07-26 09:40:15.442532: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_2207', 276 bytes spill stores, 276 bytes spill loads\n",
      "\n",
      "2025-07-26 09:40:22.997266: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_2207', 128 bytes spill stores, 128 bytes spill loads\n",
      "\n",
      "2025-07-26 09:40:23.258727: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1876', 44 bytes spill stores, 52 bytes spill loads\n",
      "\n",
      "2025-07-26 09:40:24.084362: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_2207', 68 bytes spill stores, 68 bytes spill loads\n",
      "\n",
      "2025-07-26 09:40:24.259341: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_2207', 548 bytes spill stores, 548 bytes spill loads\n",
      "\n",
      "2025-07-26 09:40:24.771981: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_2207', 4 bytes spill stores, 4 bytes spill loads\n",
      "\n",
      "2025-07-26 09:40:31.898884: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_303', 4 bytes spill stores, 4 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01/50 | Time: 1753515633.48s | Loss: 1.8611 | Acc: 0.3244 | Val Loss: 2.2303 | Val Acc: 0.1495 | LR: 1.0e-03 ‚úÖ\n",
      "Epoch 02/50 | Time: 1753515635.53s | Loss: 1.5200 | Acc: 0.4518 | Val Loss: 2.3377 | Val Acc: 0.2140 | LR: 1.0e-03 ‚úÖ\n",
      "Epoch 03/50 | Time: 1753515637.49s | Loss: 1.3419 | Acc: 0.5304 | Val Loss: 2.2469 | Val Acc: 0.2480 | LR: 1.0e-03 ‚úÖ\n",
      "Epoch 04/50 | Time: 1753515639.46s | Loss: 1.1928 | Acc: 0.5793 | Val Loss: 2.1272 | Val Acc: 0.3125 | LR: 1.0e-03 ‚úÖ\n",
      "Epoch 05/50 | Time: 1753515641.43s | Loss: 1.1110 | Acc: 0.6147 | Val Loss: 1.8609 | Val Acc: 0.3845 | LR: 1.0e-03 ‚úÖ\n",
      "Epoch 06/50 | Time: 1753515643.41s | Loss: 1.0522 | Acc: 0.6352 | Val Loss: 1.8341 | Val Acc: 0.4095 | LR: 1.0e-03 ‚úÖ\n",
      "Epoch 07/50 | Time: 1753515645.47s | Loss: 1.0046 | Acc: 0.6528 | Val Loss: 1.4228 | Val Acc: 0.5280 | LR: 1.0e-03 ‚úÖ\n",
      "Epoch 08/50 | Time: 1753515647.57s | Loss: 0.9531 | Acc: 0.6698 | Val Loss: 1.2816 | Val Acc: 0.5780 | LR: 1.0e-03 ‚úÖ\n",
      "Epoch 09/50 | Time: 1753515649.53s | Loss: 0.9226 | Acc: 0.6845 | Val Loss: 1.5723 | Val Acc: 0.5040 | LR: 1.0e-03\n",
      "Epoch 10/50 | Time: 1753515651.45s | Loss: 0.9172 | Acc: 0.6858 | Val Loss: 1.5059 | Val Acc: 0.5545 | LR: 1.0e-03\n",
      "Epoch 11/50 | Time: 1753515653.49s | Loss: 0.8417 | Acc: 0.7142 | Val Loss: 1.7378 | Val Acc: 0.5015 | LR: 1.0e-03\n",
      "Epoch 12/50 | Time: 1753515655.42s | Loss: 0.8090 | Acc: 0.7190 | Val Loss: 1.3719 | Val Acc: 0.5835 | LR: 1.0e-03 ‚úÖ\n",
      "Epoch 13/50 | Time: 1753515657.37s | Loss: 0.7989 | Acc: 0.7290 | Val Loss: 1.7405 | Val Acc: 0.5045 | LR: 1.0e-03\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "Epoch 14/50 | Time: 1753515659.34s | Loss: 0.6942 | Acc: 0.7684 | Val Loss: 1.0658 | Val Acc: 0.6310 | LR: 2.0e-04 ‚úÖ\n",
      "Epoch 15/50 | Time: 1753515661.42s | Loss: 0.6754 | Acc: 0.7758 | Val Loss: 0.9838 | Val Acc: 0.6870 | LR: 2.0e-04 ‚úÖ\n",
      "Epoch 16/50 | Time: 1753515663.42s | Loss: 0.6527 | Acc: 0.7806 | Val Loss: 1.0112 | Val Acc: 0.6700 | LR: 2.0e-04\n",
      "Epoch 17/50 | Time: 1753515665.38s | Loss: 0.6558 | Acc: 0.7781 | Val Loss: 0.9242 | Val Acc: 0.7040 | LR: 2.0e-04 ‚úÖ\n",
      "Epoch 18/50 | Time: 1753515667.33s | Loss: 0.6496 | Acc: 0.7816 | Val Loss: 0.8460 | Val Acc: 0.7365 | LR: 2.0e-04 ‚úÖ\n",
      "Epoch 19/50 | Time: 1753515669.29s | Loss: 0.6367 | Acc: 0.7846 | Val Loss: 0.8374 | Val Acc: 0.7415 | LR: 2.0e-04 ‚úÖ\n",
      "Epoch 20/50 | Time: 1753515671.31s | Loss: 0.6219 | Acc: 0.7938 | Val Loss: 0.9222 | Val Acc: 0.6905 | LR: 2.0e-04\n",
      "Epoch 21/50 | Time: 1753515673.25s | Loss: 0.6293 | Acc: 0.7902 | Val Loss: 0.9349 | Val Acc: 0.6835 | LR: 2.0e-04\n",
      "Epoch 22/50 | Time: 1753515675.15s | Loss: 0.6159 | Acc: 0.7938 | Val Loss: 0.8426 | Val Acc: 0.7225 | LR: 2.0e-04\n",
      "Epoch 23/50 | Time: 1753515677.16s | Loss: 0.6130 | Acc: 0.7950 | Val Loss: 0.9569 | Val Acc: 0.6845 | LR: 2.0e-04\n",
      "Epoch 24/50 | Time: 1753515679.17s | Loss: 0.6000 | Acc: 0.8025 | Val Loss: 0.8077 | Val Acc: 0.7455 | LR: 2.0e-04 ‚úÖ\n",
      "Epoch 25/50 | Time: 1753515681.13s | Loss: 0.5979 | Acc: 0.8000 | Val Loss: 0.9474 | Val Acc: 0.6960 | LR: 2.0e-04\n",
      "Epoch 26/50 | Time: 1753515683.03s | Loss: 0.5943 | Acc: 0.8000 | Val Loss: 1.1297 | Val Acc: 0.6450 | LR: 2.0e-04\n",
      "Epoch 27/50 | Time: 1753515685.03s | Loss: 0.5729 | Acc: 0.8045 | Val Loss: 0.8526 | Val Acc: 0.7305 | LR: 2.0e-04\n",
      "Epoch 28/50 | Time: 1753515686.99s | Loss: 0.5743 | Acc: 0.8035 | Val Loss: 1.0337 | Val Acc: 0.6705 | LR: 2.0e-04\n",
      "Epoch 29/50 | Time: 1753515688.89s | Loss: 0.5681 | Acc: 0.8077 | Val Loss: 1.2661 | Val Acc: 0.6145 | LR: 2.0e-04\n",
      "\n",
      "Epoch 29: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
      "Epoch 30/50 | Time: 1753515690.81s | Loss: 0.5488 | Acc: 0.8167 | Val Loss: 0.8089 | Val Acc: 0.7415 | LR: 4.0e-05\n",
      "Epoch 31/50 | Time: 1753515692.85s | Loss: 0.5384 | Acc: 0.8222 | Val Loss: 0.7983 | Val Acc: 0.7465 | LR: 4.0e-05 ‚úÖ\n",
      "Epoch 32/50 | Time: 1753515694.86s | Loss: 0.5263 | Acc: 0.8262 | Val Loss: 0.7729 | Val Acc: 0.7655 | LR: 4.0e-05 ‚úÖ\n",
      "Epoch 33/50 | Time: 1753515696.88s | Loss: 0.5364 | Acc: 0.8217 | Val Loss: 0.8005 | Val Acc: 0.7570 | LR: 4.0e-05\n",
      "Epoch 34/50 | Time: 1753515698.77s | Loss: 0.5249 | Acc: 0.8289 | Val Loss: 0.7977 | Val Acc: 0.7440 | LR: 4.0e-05\n",
      "Epoch 35/50 | Time: 1753515700.68s | Loss: 0.5274 | Acc: 0.8297 | Val Loss: 0.7889 | Val Acc: 0.7565 | LR: 4.0e-05\n",
      "Epoch 36/50 | Time: 1753515702.66s | Loss: 0.5241 | Acc: 0.8279 | Val Loss: 0.7696 | Val Acc: 0.7660 | LR: 4.0e-05 ‚úÖ\n",
      "Epoch 37/50 | Time: 1753515704.74s | Loss: 0.5229 | Acc: 0.8210 | Val Loss: 0.7822 | Val Acc: 0.7565 | LR: 4.0e-05\n",
      "Epoch 38/50 | Time: 1753515706.73s | Loss: 0.5451 | Acc: 0.8162 | Val Loss: 0.7695 | Val Acc: 0.7600 | LR: 4.0e-05\n",
      "Epoch 39/50 | Time: 1753515708.70s | Loss: 0.5297 | Acc: 0.8207 | Val Loss: 0.7627 | Val Acc: 0.7645 | LR: 4.0e-05\n",
      "Epoch 40/50 | Time: 1753515710.66s | Loss: 0.5267 | Acc: 0.8247 | Val Loss: 0.7598 | Val Acc: 0.7590 | LR: 4.0e-05\n",
      "Epoch 41/50 | Time: 1753515712.63s | Loss: 0.5207 | Acc: 0.8279 | Val Loss: 0.7711 | Val Acc: 0.7655 | LR: 4.0e-05\n",
      "Epoch 42/50 | Time: 1753515714.59s | Loss: 0.5185 | Acc: 0.8277 | Val Loss: 0.7647 | Val Acc: 0.7665 | LR: 4.0e-05 ‚úÖ\n",
      "Epoch 43/50 | Time: 1753515716.59s | Loss: 0.5023 | Acc: 0.8367 | Val Loss: 0.7918 | Val Acc: 0.7440 | LR: 4.0e-05\n",
      "Epoch 44/50 | Time: 1753515718.54s | Loss: 0.5233 | Acc: 0.8262 | Val Loss: 0.7762 | Val Acc: 0.7515 | LR: 4.0e-05\n",
      "Epoch 45/50 | Time: 1753515720.50s | Loss: 0.5236 | Acc: 0.8234 | Val Loss: 0.7697 | Val Acc: 0.7650 | LR: 4.0e-05\n",
      "\n",
      "Epoch 45: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.\n",
      "Epoch 46/50 | Time: 1753515722.45s | Loss: 0.5154 | Acc: 0.8307 | Val Loss: 0.7739 | Val Acc: 0.7690 | LR: 8.0e-06 ‚úÖ\n",
      "Epoch 47/50 | Time: 1753515724.40s | Loss: 0.5059 | Acc: 0.8307 | Val Loss: 0.7670 | Val Acc: 0.7650 | LR: 8.0e-06\n",
      "Epoch 48/50 | Time: 1753515726.31s | Loss: 0.5054 | Acc: 0.8321 | Val Loss: 0.7624 | Val Acc: 0.7695 | LR: 8.0e-06 ‚úÖ\n",
      "Epoch 49/50 | Time: 1753515728.39s | Loss: 0.5142 | Acc: 0.8329 | Val Loss: 0.7673 | Val Acc: 0.7635 | LR: 8.0e-06\n",
      "Epoch 50/50 | Time: 1753515730.39s | Loss: 0.4953 | Acc: 0.8394 | Val Loss: 0.7611 | Val Acc: 0.7680 | LR: 8.0e-06\n",
      "\n",
      "Epoch 50: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.\n",
      "üèÅ Finished training. Best Validation Accuracy: 0.7695\n",
      "\n",
      "================================================================================\n",
      "TRAINING ARCHITECTURE: 'SeparableResSE_CNN'\n",
      "================================================================================\n",
      "üöÄ Starting training for model: SeparableResSE_CNN...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-26 09:42:33.376369: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_7', 8 bytes spill stores, 8 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01/50 | Time: 1753515766.52s | Loss: 1.8437 | Acc: 0.3301 | Val Loss: 2.3012 | Val Acc: 0.1260 | LR: 1.0e-03 ‚úÖ\n",
      "Epoch 02/50 | Time: 1753515769.75s | Loss: 1.3583 | Acc: 0.5085 | Val Loss: 2.3672 | Val Acc: 0.1205 | LR: 1.0e-03\n",
      "Epoch 03/50 | Time: 1753515772.74s | Loss: 1.1632 | Acc: 0.5880 | Val Loss: 2.6775 | Val Acc: 0.1115 | LR: 1.0e-03\n",
      "Epoch 04/50 | Time: 1753515775.80s | Loss: 1.0355 | Acc: 0.6367 | Val Loss: 3.3661 | Val Acc: 0.1940 | LR: 1.0e-03 ‚úÖ\n",
      "Epoch 05/50 | Time: 1753515779.10s | Loss: 0.9242 | Acc: 0.6731 | Val Loss: 2.4726 | Val Acc: 0.3220 | LR: 1.0e-03 ‚úÖ\n",
      "Epoch 06/50 | Time: 1753515782.28s | Loss: 0.8303 | Acc: 0.7073 | Val Loss: 1.8466 | Val Acc: 0.4620 | LR: 1.0e-03 ‚úÖ\n",
      "Epoch 07/50 | Time: 1753515785.53s | Loss: 0.7516 | Acc: 0.7409 | Val Loss: 1.7295 | Val Acc: 0.5090 | LR: 1.0e-03 ‚úÖ\n",
      "Epoch 08/50 | Time: 1753515788.84s | Loss: 0.6754 | Acc: 0.7663 | Val Loss: 1.1834 | Val Acc: 0.6200 | LR: 1.0e-03 ‚úÖ\n",
      "Epoch 09/50 | Time: 1753515792.01s | Loss: 0.6066 | Acc: 0.7940 | Val Loss: 1.4305 | Val Acc: 0.6365 | LR: 1.0e-03 ‚úÖ\n",
      "Epoch 10/50 | Time: 1753515795.23s | Loss: 0.5411 | Acc: 0.8139 | Val Loss: 1.3862 | Val Acc: 0.6075 | LR: 1.0e-03\n",
      "Epoch 11/50 | Time: 1753515798.21s | Loss: 0.4889 | Acc: 0.8336 | Val Loss: 1.4859 | Val Acc: 0.6110 | LR: 1.0e-03\n",
      "Epoch 12/50 | Time: 1753515801.28s | Loss: 0.4205 | Acc: 0.8581 | Val Loss: 1.8389 | Val Acc: 0.5605 | LR: 1.0e-03\n",
      "Epoch 13/50 | Time: 1753515804.34s | Loss: 0.3888 | Acc: 0.8686 | Val Loss: 1.2617 | Val Acc: 0.6515 | LR: 1.0e-03 ‚úÖ\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "Epoch 14/50 | Time: 1753515807.47s | Loss: 0.2770 | Acc: 0.9117 | Val Loss: 1.0495 | Val Acc: 0.7195 | LR: 2.0e-04 ‚úÖ\n",
      "Epoch 15/50 | Time: 1753515810.51s | Loss: 0.2435 | Acc: 0.9272 | Val Loss: 0.9159 | Val Acc: 0.7335 | LR: 2.0e-04 ‚úÖ\n",
      "Epoch 16/50 | Time: 1753515813.66s | Loss: 0.2249 | Acc: 0.9341 | Val Loss: 0.9770 | Val Acc: 0.7175 | LR: 2.0e-04\n",
      "Epoch 17/50 | Time: 1753515816.67s | Loss: 0.2092 | Acc: 0.9391 | Val Loss: 0.9835 | Val Acc: 0.7260 | LR: 2.0e-04\n",
      "Epoch 18/50 | Time: 1753515819.63s | Loss: 0.2018 | Acc: 0.9407 | Val Loss: 1.0198 | Val Acc: 0.7230 | LR: 2.0e-04\n",
      "Epoch 19/50 | Time: 1753515822.70s | Loss: 0.1901 | Acc: 0.9451 | Val Loss: 0.9742 | Val Acc: 0.7330 | LR: 2.0e-04\n",
      "Epoch 20/50 | Time: 1753515825.70s | Loss: 0.1820 | Acc: 0.9476 | Val Loss: 0.9783 | Val Acc: 0.7245 | LR: 2.0e-04\n",
      "\n",
      "Epoch 20: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
      "Epoch 21/50 | Time: 1753515828.67s | Loss: 0.1678 | Acc: 0.9497 | Val Loss: 0.8985 | Val Acc: 0.7425 | LR: 4.0e-05 ‚úÖ\n",
      "Epoch 22/50 | Time: 1753515831.82s | Loss: 0.1610 | Acc: 0.9554 | Val Loss: 0.9140 | Val Acc: 0.7460 | LR: 4.0e-05 ‚úÖ\n",
      "Epoch 23/50 | Time: 1753515835.02s | Loss: 0.1559 | Acc: 0.9604 | Val Loss: 0.9240 | Val Acc: 0.7385 | LR: 4.0e-05\n",
      "Epoch 24/50 | Time: 1753515837.99s | Loss: 0.1579 | Acc: 0.9561 | Val Loss: 0.9261 | Val Acc: 0.7390 | LR: 4.0e-05\n",
      "Epoch 25/50 | Time: 1753515841.05s | Loss: 0.1505 | Acc: 0.9586 | Val Loss: 0.9152 | Val Acc: 0.7415 | LR: 4.0e-05\n",
      "Epoch 26/50 | Time: 1753515844.07s | Loss: 0.1499 | Acc: 0.9589 | Val Loss: 0.9264 | Val Acc: 0.7400 | LR: 4.0e-05\n",
      "\n",
      "Epoch 26: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.\n",
      "Epoch 27/50 | Time: 1753515847.06s | Loss: 0.1543 | Acc: 0.9553 | Val Loss: 0.9169 | Val Acc: 0.7350 | LR: 8.0e-06\n",
      "Epoch 28/50 | Time: 1753515850.20s | Loss: 0.1500 | Acc: 0.9601 | Val Loss: 0.9190 | Val Acc: 0.7375 | LR: 8.0e-06\n",
      "Epoch 29/50 | Time: 1753515853.15s | Loss: 0.1463 | Acc: 0.9629 | Val Loss: 0.9202 | Val Acc: 0.7370 | LR: 8.0e-06\n",
      "Epoch 30/50 | Time: 1753515856.20s | Loss: 0.1467 | Acc: 0.9604 | Val Loss: 0.9209 | Val Acc: 0.7415 | LR: 8.0e-06\n",
      "Epoch 31/50 | Time: 1753515859.28s | Loss: 0.1529 | Acc: 0.9568 | Val Loss: 0.9205 | Val Acc: 0.7375 | LR: 8.0e-06\n",
      "\n",
      "Epoch 31: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.\n",
      "Epoch 32/50 | Time: 1753515862.30s | Loss: 0.1428 | Acc: 0.9601 | Val Loss: 0.9213 | Val Acc: 0.7380 | LR: 1.6e-06\n",
      "Epoch 33/50 | Time: 1753515865.26s | Loss: 0.1415 | Acc: 0.9658 | Val Loss: 0.9226 | Val Acc: 0.7385 | LR: 1.6e-06\n",
      "Epoch 34/50 | Time: 1753515868.32s | Loss: 0.1442 | Acc: 0.9614 | Val Loss: 0.9221 | Val Acc: 0.7380 | LR: 1.6e-06\n",
      "Epoch 35/50 | Time: 1753515871.39s | Loss: 0.1418 | Acc: 0.9608 | Val Loss: 0.9200 | Val Acc: 0.7390 | LR: 1.6e-06\n",
      "Epoch 36/50 | Time: 1753515874.45s | Loss: 0.1461 | Acc: 0.9611 | Val Loss: 0.9226 | Val Acc: 0.7385 | LR: 1.6e-06\n",
      "\n",
      "Epoch 36: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.\n",
      "Epoch 37/50 | Time: 1753515877.42s | Loss: 0.1466 | Acc: 0.9593 | Val Loss: 0.9229 | Val Acc: 0.7385 | LR: 3.2e-07\n",
      "üèÅ Finished training. Best Validation Accuracy: 0.7460\n",
      "\n",
      "================================================================================\n",
      "TRAINING ARCHITECTURE: 'ResSE_AudioCNN'\n",
      "================================================================================\n",
      "üöÄ Starting training for model: ResSE_AudioCNN...\n",
      "Epoch 01/50 | Time: 1753515904.77s | Loss: 1.6481 | Acc: 0.4015 | Val Loss: 3.6176 | Val Acc: 0.1210 | LR: 1.0e-03 ‚úÖ\n",
      "Epoch 02/50 | Time: 1753515908.59s | Loss: 1.2008 | Acc: 0.5765 | Val Loss: 2.9935 | Val Acc: 0.2530 | LR: 1.0e-03 ‚úÖ\n",
      "Epoch 03/50 | Time: 1753515912.46s | Loss: 1.0059 | Acc: 0.6457 | Val Loss: 2.8575 | Val Acc: 0.3040 | LR: 1.0e-03 ‚úÖ\n",
      "Epoch 04/50 | Time: 1753515916.36s | Loss: 0.8024 | Acc: 0.7250 | Val Loss: 2.2313 | Val Acc: 0.3965 | LR: 1.0e-03 ‚úÖ\n",
      "Epoch 05/50 | Time: 1753515920.26s | Loss: 0.6727 | Acc: 0.7783 | Val Loss: 1.5883 | Val Acc: 0.5650 | LR: 1.0e-03 ‚úÖ\n",
      "Epoch 06/50 | Time: 1753515924.24s | Loss: 0.5767 | Acc: 0.8067 | Val Loss: 1.9185 | Val Acc: 0.5105 | LR: 1.0e-03\n",
      "Epoch 07/50 | Time: 1753515927.99s | Loss: 0.4602 | Acc: 0.8412 | Val Loss: 2.1549 | Val Acc: 0.5105 | LR: 1.0e-03\n",
      "Epoch 08/50 | Time: 1753515931.68s | Loss: 0.3834 | Acc: 0.8740 | Val Loss: 3.3180 | Val Acc: 0.4540 | LR: 1.0e-03\n",
      "Epoch 09/50 | Time: 1753515935.42s | Loss: 0.3437 | Acc: 0.8858 | Val Loss: 2.0328 | Val Acc: 0.5295 | LR: 1.0e-03\n",
      "Epoch 10/50 | Time: 1753515939.12s | Loss: 0.2332 | Acc: 0.9274 | Val Loss: 2.1133 | Val Acc: 0.5220 | LR: 1.0e-03\n",
      "\n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "Epoch 11/50 | Time: 1753515942.88s | Loss: 0.1383 | Acc: 0.9619 | Val Loss: 0.7891 | Val Acc: 0.7655 | LR: 2.0e-04 ‚úÖ\n",
      "Epoch 12/50 | Time: 1753515946.78s | Loss: 0.1004 | Acc: 0.9768 | Val Loss: 0.8275 | Val Acc: 0.7610 | LR: 2.0e-04\n",
      "Epoch 13/50 | Time: 1753515950.50s | Loss: 0.0921 | Acc: 0.9783 | Val Loss: 0.8638 | Val Acc: 0.7600 | LR: 2.0e-04\n",
      "Epoch 14/50 | Time: 1753515954.23s | Loss: 0.0790 | Acc: 0.9811 | Val Loss: 0.9509 | Val Acc: 0.7355 | LR: 2.0e-04\n",
      "Epoch 15/50 | Time: 1753515957.87s | Loss: 0.0722 | Acc: 0.9836 | Val Loss: 1.0730 | Val Acc: 0.7070 | LR: 2.0e-04\n",
      "Epoch 16/50 | Time: 1753515961.65s | Loss: 0.0624 | Acc: 0.9853 | Val Loss: 0.8595 | Val Acc: 0.7775 | LR: 2.0e-04 ‚úÖ\n",
      "\n",
      "Epoch 16: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
      "Epoch 17/50 | Time: 1753515965.54s | Loss: 0.0521 | Acc: 0.9900 | Val Loss: 0.7925 | Val Acc: 0.7825 | LR: 4.0e-05 ‚úÖ\n",
      "Epoch 18/50 | Time: 1753515969.41s | Loss: 0.0475 | Acc: 0.9928 | Val Loss: 0.7953 | Val Acc: 0.7785 | LR: 4.0e-05\n",
      "Epoch 19/50 | Time: 1753515973.09s | Loss: 0.0530 | Acc: 0.9885 | Val Loss: 0.8065 | Val Acc: 0.7835 | LR: 4.0e-05 ‚úÖ\n",
      "Epoch 20/50 | Time: 1753515976.95s | Loss: 0.0483 | Acc: 0.9915 | Val Loss: 0.7839 | Val Acc: 0.7835 | LR: 4.0e-05\n",
      "Epoch 21/50 | Time: 1753515980.71s | Loss: 0.0448 | Acc: 0.9923 | Val Loss: 0.8611 | Val Acc: 0.7770 | LR: 4.0e-05\n",
      "Epoch 22/50 | Time: 1753515984.37s | Loss: 0.0451 | Acc: 0.9927 | Val Loss: 0.8562 | Val Acc: 0.7795 | LR: 4.0e-05\n",
      "Epoch 23/50 | Time: 1753515988.10s | Loss: 0.0431 | Acc: 0.9935 | Val Loss: 0.8047 | Val Acc: 0.7865 | LR: 4.0e-05 ‚úÖ\n",
      "Epoch 24/50 | Time: 1753515991.99s | Loss: 0.0410 | Acc: 0.9933 | Val Loss: 0.8224 | Val Acc: 0.7850 | LR: 4.0e-05\n",
      "Epoch 25/50 | Time: 1753515995.74s | Loss: 0.0401 | Acc: 0.9935 | Val Loss: 0.8213 | Val Acc: 0.7855 | LR: 4.0e-05\n",
      "\n",
      "Epoch 25: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.\n",
      "Epoch 26/50 | Time: 1753515999.47s | Loss: 0.0375 | Acc: 0.9947 | Val Loss: 0.8237 | Val Acc: 0.7865 | LR: 8.0e-06\n",
      "Epoch 27/50 | Time: 1753516003.13s | Loss: 0.0394 | Acc: 0.9940 | Val Loss: 0.8226 | Val Acc: 0.7875 | LR: 8.0e-06 ‚úÖ\n",
      "Epoch 28/50 | Time: 1753516007.09s | Loss: 0.0390 | Acc: 0.9922 | Val Loss: 0.8215 | Val Acc: 0.7910 | LR: 8.0e-06 ‚úÖ\n",
      "Epoch 29/50 | Time: 1753516010.97s | Loss: 0.0386 | Acc: 0.9940 | Val Loss: 0.8178 | Val Acc: 0.7860 | LR: 8.0e-06\n",
      "Epoch 30/50 | Time: 1753516014.72s | Loss: 0.0380 | Acc: 0.9953 | Val Loss: 0.8236 | Val Acc: 0.7855 | LR: 8.0e-06\n",
      "\n",
      "Epoch 30: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.\n",
      "Epoch 31/50 | Time: 1753516018.38s | Loss: 0.0412 | Acc: 0.9928 | Val Loss: 0.8178 | Val Acc: 0.7860 | LR: 1.6e-06\n",
      "Epoch 32/50 | Time: 1753516022.12s | Loss: 0.0355 | Acc: 0.9935 | Val Loss: 0.8176 | Val Acc: 0.7855 | LR: 1.6e-06\n",
      "Epoch 33/50 | Time: 1753516025.86s | Loss: 0.0392 | Acc: 0.9930 | Val Loss: 0.8178 | Val Acc: 0.7860 | LR: 1.6e-06\n",
      "Epoch 34/50 | Time: 1753516029.51s | Loss: 0.0400 | Acc: 0.9928 | Val Loss: 0.8151 | Val Acc: 0.7855 | LR: 1.6e-06\n",
      "Epoch 35/50 | Time: 1753516033.26s | Loss: 0.0374 | Acc: 0.9933 | Val Loss: 0.8145 | Val Acc: 0.7845 | LR: 1.6e-06\n",
      "\n",
      "Epoch 35: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.\n",
      "Epoch 36/50 | Time: 1753516037.02s | Loss: 0.0373 | Acc: 0.9945 | Val Loss: 0.8139 | Val Acc: 0.7845 | LR: 3.2e-07\n",
      "Epoch 37/50 | Time: 1753516040.68s | Loss: 0.0378 | Acc: 0.9945 | Val Loss: 0.8139 | Val Acc: 0.7845 | LR: 3.2e-07\n",
      "Epoch 38/50 | Time: 1753516044.43s | Loss: 0.0379 | Acc: 0.9942 | Val Loss: 0.8150 | Val Acc: 0.7860 | LR: 3.2e-07\n",
      "Epoch 39/50 | Time: 1753516048.18s | Loss: 0.0357 | Acc: 0.9953 | Val Loss: 0.8137 | Val Acc: 0.7865 | LR: 3.2e-07\n",
      "Epoch 40/50 | Time: 1753516051.88s | Loss: 0.0383 | Acc: 0.9932 | Val Loss: 0.8139 | Val Acc: 0.7865 | LR: 3.2e-07\n",
      "\n",
      "Epoch 40: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-08.\n",
      "Epoch 41/50 | Time: 1753516055.62s | Loss: 0.0380 | Acc: 0.9935 | Val Loss: 0.8157 | Val Acc: 0.7860 | LR: 6.4e-08\n",
      "Epoch 42/50 | Time: 1753516059.38s | Loss: 0.0381 | Acc: 0.9938 | Val Loss: 0.8153 | Val Acc: 0.7860 | LR: 6.4e-08\n",
      "Epoch 43/50 | Time: 1753516063.12s | Loss: 0.0380 | Acc: 0.9942 | Val Loss: 0.8150 | Val Acc: 0.7855 | LR: 6.4e-08\n",
      "üèÅ Finished training. Best Validation Accuracy: 0.7910\n",
      "\n",
      "üéâ FINAL COMPARATIVE ANALYSIS COMPLETED üéâ\n",
      "\n",
      "Final Leaderboard:\n",
      "| Model              |   Test_Accuracy |   Best_Val_Accuracy |   Epochs_Run |\n",
      "|:-------------------|----------------:|--------------------:|-------------:|\n",
      "| ResSE_AudioCNN     |          0.78   |              0.791  |           43 |\n",
      "| SE_AudioCNN        |          0.7345 |              0.7695 |           50 |\n",
      "| SeparableResSE_CNN |          0.7435 |              0.746  |           37 |\n",
      "| PaperCNN_Lite      |          0.6985 |              0.7275 |           50 |\n",
      "| Efficient_VGG      |          0.6875 |              0.7125 |           38 |\n"
     ]
    }
   ],
   "source": [
    "# ===================================================================\n",
    "# CELL 3: DEFINITIVE COMPARATIVE ANALYSIS FRAMEWORK\n",
    "# ===================================================================\n",
    "# This cell orchestrates the final comparative analysis. It loads the\n",
    "# pre-processed data, defines robust data pipelines with corrected\n",
    "# augmentation, and systematically trains and evaluates all candidate\n",
    "# architectures using a professional custom logger for clear progress tracking.\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import traceback\n",
    "import numpy as np\n",
    "import pickle\n",
    "import time\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import optimizers, callbacks\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 0. DATA LOADING AND PREPARATION\n",
    "# -------------------------------------------------------------------\n",
    "print(\"Loading pre-processed data...\")\n",
    "try:\n",
    "    X_train = np.load(os.path.join(PROCESSED_DATA_PATH, 'X_train.npy'))\n",
    "    y_train = np.load(os.path.join(PROCESSED_DATA_PATH, 'y_train.npy'))\n",
    "    X_val = np.load(os.path.join(PROCESSED_DATA_PATH, 'X_val.npy'))\n",
    "    y_val = np.load(os.path.join(PROCESSED_DATA_PATH, 'y_val.npy'))\n",
    "    X_test = np.load(os.path.join(PROCESSED_DATA_PATH, 'X_test.npy'))\n",
    "    y_test = np.load(os.path.join(PROCESSED_DATA_PATH, 'y_test.npy'))\n",
    "    with open(os.path.join(PROCESSED_DATA_PATH, 'label_encoder.pkl'), 'rb') as f:\n",
    "        label_encoder = pickle.load(f)\n",
    "\n",
    "    y_train_cat = to_categorical(y_train)\n",
    "    y_val_cat = to_categorical(y_val)\n",
    "    y_test_cat = to_categorical(y_test)\n",
    "    \n",
    "    print(\"‚úÖ Data successfully loaded and prepared.\")\n",
    "except FileNotFoundError:\n",
    "    raise RuntimeError(\"ERROR: Data files not found. Please run the '00_Data_Preprocessing' notebook first.\")\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 1. DATA AUGMENTATION & CUSTOM CALLBACK\n",
    "# -------------------------------------------------------------------\n",
    "@tf.function\n",
    "def spec_augment_tf(spectrogram, label):\n",
    "    \"\"\"\n",
    "    Applies frequency and time masking to a spectrogram.\n",
    "    *** BUG FIX: Corrected variable names from 'mask_values' to the\n",
    "    *** correctly scoped 'mask_freq_values' and 'mask_time_values'.\n",
    "    \"\"\"\n",
    "    aug_spec = tf.identity(spectrogram)\n",
    "    \n",
    "    # Frequency Masking\n",
    "    freq_bins = tf.shape(aug_spec)[0]\n",
    "    f_param = tf.cast(tf.cast(freq_bins, tf.float32) * 0.2, tf.int32)\n",
    "    if f_param > 1:\n",
    "        f = tf.random.uniform(shape=(), minval=1, maxval=f_param, dtype=tf.int32)\n",
    "        f0 = tf.random.uniform(shape=(), minval=0, maxval=freq_bins - f, dtype=tf.int32)\n",
    "        mask_freq_values = tf.concat([tf.ones((f0,)), tf.zeros((f,)), tf.ones((freq_bins - f0 - f,))], axis=0)\n",
    "        freq_mask = tf.reshape(tf.cast(mask_freq_values, aug_spec.dtype), (freq_bins, 1, 1))\n",
    "        aug_spec *= freq_mask\n",
    "\n",
    "    # Time Masking\n",
    "    time_steps = tf.shape(aug_spec)[1]\n",
    "    t_param = tf.cast(tf.cast(time_steps, tf.float32) * 0.2, tf.int32)\n",
    "    if t_param > 1:\n",
    "        t = tf.random.uniform(shape=(), minval=1, maxval=t_param, dtype=tf.int32)\n",
    "        t0 = tf.random.uniform(shape=(), minval=0, maxval=time_steps - t, dtype=tf.int32)\n",
    "        mask_time_values = tf.concat([tf.ones((t0,)), tf.zeros((t,)), tf.ones((time_steps - t0 - t,))], axis=0)\n",
    "        time_mask = tf.reshape(tf.cast(mask_time_values, aug_spec.dtype), (1, time_steps, 1))\n",
    "        aug_spec *= time_mask\n",
    "        \n",
    "    return aug_spec, label\n",
    "\n",
    "class RichLoggerCallback(callbacks.Callback):\n",
    "    \"\"\"A custom Keras callback for clean, informative, and professional logging.\"\"\"\n",
    "    def __init__(self, total_epochs):\n",
    "        super().__init__()\n",
    "        self.total_epochs = total_epochs\n",
    "        self.best_val_accuracy = 0\n",
    "        self.epoch_start_time = 0\n",
    "\n",
    "    def on_train_begin(self, logs=None):\n",
    "        print(f\"üöÄ Starting training for model: {self.model.name}...\")\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        epoch_time = time.time() - self.epoch_start_time\n",
    "        lr = self.model.optimizer.learning_rate\n",
    "        \n",
    "        # Handle potential learning rate schedules\n",
    "        if isinstance(lr, tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "            lr = lr(self.model.optimizer.iterations)\n",
    "\n",
    "        # *** BUG FIX: Convert the learning rate variable to a numpy value before formatting.\n",
    "        lr_value = lr.numpy() if hasattr(lr, 'numpy') else lr\n",
    "\n",
    "        is_best = \"\"\n",
    "        if logs['val_accuracy'] > self.best_val_accuracy:\n",
    "            self.best_val_accuracy = logs['val_accuracy']\n",
    "            is_best = \" ‚úÖ\"\n",
    "\n",
    "        log_str = (\n",
    "            f\"Epoch {epoch + 1:02d}/{self.total_epochs} | \"\n",
    "            f\"Time: {epoch_time:.2f}s | \"\n",
    "            f\"Loss: {logs['loss']:.4f} | Acc: {logs['accuracy']:.4f} | \"\n",
    "            f\"Val Loss: {logs['val_loss']:.4f} | Val Acc: {logs['val_accuracy']:.4f} | \"\n",
    "            f\"LR: {lr_value:.1e}{is_best}\"\n",
    "        )\n",
    "        print(log_str)\n",
    "\n",
    "    def on_train_end(self, logs=None):\n",
    "        print(f\"üèÅ Finished training. Best Validation Accuracy: {self.best_val_accuracy:.4f}\")\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 2. EXPERIMENT ORCHESTRATION CLASS\n",
    "# -------------------------------------------------------------------\n",
    "class ModelEvaluator:\n",
    "    \"\"\"Orchestrates the training and evaluation of multiple models.\"\"\"\n",
    "    def __init__(self):\n",
    "        self.results = []\n",
    "\n",
    "    def run_experiments(self, model_factories, train_data, val_data, test_data, epochs):\n",
    "        for model_name, model_factory_fn in model_factories.items():\n",
    "            print(f\"\\n{'='*80}\\nTRAINING ARCHITECTURE: '{model_name}'\\n{'='*80}\")\n",
    "            try:\n",
    "                model = model_factory_fn()\n",
    "                optimizer = keras.optimizers.Adam(learning_rate=1e-3)\n",
    "                model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "                \n",
    "                callbacks_list = [\n",
    "                    RichLoggerCallback(total_epochs=epochs),\n",
    "                    callbacks.EarlyStopping(monitor='val_accuracy', patience=15, restore_best_weights=True, verbose=0),\n",
    "                    callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, verbose=1),\n",
    "                    callbacks.ModelCheckpoint(os.path.join(MODELS_PATH, f\"{model_name}_best.keras\"), \n",
    "                                              monitor='val_accuracy', save_best_only=True, verbose=0)\n",
    "                ]\n",
    "                \n",
    "                history = model.fit(train_data, epochs=epochs, validation_data=val_data, callbacks=callbacks_list, verbose=0)\n",
    "                \n",
    "                test_loss, test_acc = model.evaluate(test_data, verbose=0)\n",
    "                self.results.append({\n",
    "                    'Model': model_name,\n",
    "                    'Test_Accuracy': test_acc,\n",
    "                    'Best_Val_Accuracy': max(history.history['val_accuracy']),\n",
    "                    'Epochs_Run': len(history.history['val_accuracy']),\n",
    "                })\n",
    "            except Exception:\n",
    "                print(f\"‚ùå ERROR during training of [{model_name}]:\")\n",
    "                traceback.print_exc()\n",
    "        return pd.DataFrame(self.results)\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 3. EXPERIMENT CONFIGURATION & EXECUTION\n",
    "# -------------------------------------------------------------------\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 50\n",
    "\n",
    "keras.mixed_precision.set_global_policy('float32')\n",
    "\n",
    "# Data Pipelines (without .cache() to ensure no OOM errors)\n",
    "print(\"\\nConfiguring JIT data pipelines (without caching)...\")\n",
    "train_pipeline = (tf.data.Dataset.from_tensor_slices((X_train, y_train_cat)).shuffle(len(X_train))\n",
    "                  .map(spec_augment_tf, num_parallel_calls=AUTOTUNE).batch(BATCH_SIZE).prefetch(AUTOTUNE))\n",
    "val_pipeline = (tf.data.Dataset.from_tensor_slices((X_val, y_val_cat)).batch(BATCH_SIZE).prefetch(AUTOTUNE))\n",
    "test_pipeline = (tf.data.Dataset.from_tensor_slices((X_test, y_test_cat)).batch(BATCH_SIZE).prefetch(AUTOTUNE))\n",
    "\n",
    "# Define the models for the final comparative analysis\n",
    "input_shape = X_train.shape[1:]\n",
    "num_classes = y_train_cat.shape[1]\n",
    "model_factories = {\n",
    "    'Efficient_VGG': lambda: ModelFactory.build_efficient_vgg(input_shape, num_classes),\n",
    "    'PaperCNN_Lite': lambda: ModelFactory.build_paper_cnn_lite(input_shape, num_classes),\n",
    "    'SE_AudioCNN': lambda: ModelFactory.build_se_audio_cnn(input_shape, num_classes),\n",
    "    'SeparableResSE_CNN': lambda: ModelFactory.build_separable_res_se_cnn(input_shape, num_classes),\n",
    "    'ResSE_AudioCNN': lambda: ModelFactory.build_res_se_audio_cnn(input_shape, num_classes),\n",
    "}\n",
    "\n",
    "# Execute the comparative analysis\n",
    "evaluator = ModelEvaluator()\n",
    "results_df = evaluator.run_experiments(\n",
    "    model_factories, train_pipeline, val_pipeline, test_pipeline, EPOCHS\n",
    ")\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 4. REPORTING\n",
    "# -------------------------------------------------------------------\n",
    "if not results_df.empty:\n",
    "    results_df.to_csv(os.path.join(REPORTS_PATH, 'training_summary_FINAL_COMPARISON.csv'), index=False)\n",
    "    print(\"\\nüéâ FINAL COMPARATIVE ANALYSIS COMPLETED üéâ\")\n",
    "    print(\"\\nFinal Leaderboard:\")\n",
    "    print(results_df.sort_values(by='Best_Val_Accuracy', ascending=False).to_markdown(index=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
