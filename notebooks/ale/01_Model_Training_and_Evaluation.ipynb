{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7fe0892b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-26 12:34:27.192981: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-07-26 12:34:27.388094: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1753526067.467309    6655 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1753526067.488913    6655 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1753526067.650381    6655 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1753526067.650414    6655 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1753526067.650417    6655 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1753526067.650419    6655 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-07-26 12:34:27.669337: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ GPU(s) Trovata/e: ['NVIDIA GeForce RTX 4070']\n",
      "‚úÖ Politica di Mixed Precision impostata su: mixed_float16\n",
      "\n",
      "üîÑ Caricamento dei dati pre-processati...\n",
      "\n",
      "‚úÖ Dati caricati con successo.\n",
      "   - Shape X_train: (5990, 128, 128, 1) | Shape y_train_cat: (5990, 10)\n",
      "   - Numero di classi: 10\n"
     ]
    }
   ],
   "source": [
    "# ===================================================================\n",
    "# CELLA 1: SETUP, IMPORTS E CARICAMENTO DATI\n",
    "# ===================================================================\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "import traceback\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras as keras\n",
    "from keras import layers, models, optimizers, callbacks, regularizers\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "# --- Configurazione Globale ---\n",
    "PROCESSED_DATA_PATH = '../../data/processed/'\n",
    "MODELS_PATH = '../../models/ale/'\n",
    "REPORTS_PATH = '../../reports/'\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "os.makedirs(MODELS_PATH, exist_ok=True)\n",
    "os.makedirs(REPORTS_PATH, exist_ok=True)\n",
    "\n",
    "# 1. GPU e Mixed Precision\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus: tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        print(f\"‚úÖ GPU(s) Trovata/e: {[tf.config.experimental.get_device_details(g)['device_name'] for g in gpus]}\")\n",
    "        policy = keras.mixed_precision.Policy('mixed_float16')\n",
    "        keras.mixed_precision.set_global_policy(policy)\n",
    "        print(f\"‚úÖ Politica di Mixed Precision impostata su: {keras.mixed_precision.global_policy().name}\")\n",
    "    except RuntimeError as e: print(f\"‚ö†Ô∏è Errore durante l'inizializzazione della GPU: {e}\")\n",
    "else: print(\"‚ùå NESSUNA GPU TROVATA. L'allenamento sar√† su CPU.\")\n",
    "\n",
    "# 2. Caricamento Dati Pre-processati\n",
    "print(\"\\nüîÑ Caricamento dei dati pre-processati...\")\n",
    "try:\n",
    "    X_train = np.load(os.path.join(PROCESSED_DATA_PATH, 'X_train.npy'))\n",
    "    y_train = np.load(os.path.join(PROCESSED_DATA_PATH, 'y_train.npy'))\n",
    "    X_val = np.load(os.path.join(PROCESSED_DATA_PATH, 'X_val.npy'))\n",
    "    y_val = np.load(os.path.join(PROCESSED_DATA_PATH, 'y_val.npy'))\n",
    "    X_test = np.load(os.path.join(PROCESSED_DATA_PATH, 'X_test.npy'))\n",
    "    y_test = np.load(os.path.join(PROCESSED_DATA_PATH, 'y_test.npy'))\n",
    "    \n",
    "    with open(os.path.join(PROCESSED_DATA_PATH, 'label_encoder.pkl'), 'rb') as f:\n",
    "        label_encoder = pickle.load(f)\n",
    "\n",
    "    # Conversione in formato categorico\n",
    "    num_classes = len(label_encoder.classes_)\n",
    "    y_train_cat = to_categorical(y_train, num_classes=num_classes)\n",
    "    y_val_cat = to_categorical(y_val, num_classes=num_classes)\n",
    "    y_test_cat = to_categorical(y_test, num_classes=num_classes)\n",
    "    \n",
    "    print(\"\\n‚úÖ Dati caricati con successo.\")\n",
    "    print(f\"   - Shape X_train: {X_train.shape} | Shape y_train_cat: {y_train_cat.shape}\")\n",
    "    print(f\"   - Numero di classi: {num_classes}\")\n",
    "except FileNotFoundError:\n",
    "    print(\"‚ùå ERRORE: File di dati non trovati. Eseguire prima il notebook '00_Setup_and_Data_Preparation.ipynb'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56958c89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ ModelFactory defined with 5 candidate architectures for the final comparative analysis.\n"
     ]
    }
   ],
   "source": [
    "# ===================================================================\n",
    "# CELL 2: COMPREHENSIVE MODEL FACTORY FOR COMPARATIVE ANALYSIS\n",
    "# ===================================================================\n",
    "# This cell defines all candidate architectures for our final analysis.\n",
    "# The selection progresses from simple and efficient baselines to more\n",
    "# complex residual networks, allowing for a thorough evaluation of\n",
    "# architectural trade-offs. All code is written in English for clarity.\n",
    "\n",
    "from keras import layers, models, regularizers\n",
    "\n",
    "class ModelFactory:\n",
    "    \"\"\"\n",
    "    A comprehensive factory for building and comparing a curated set of CNN architectures.\n",
    "    This class centralizes our key models for the final comparative experiment.\n",
    "    \"\"\"\n",
    "\n",
    "    # -------------------------------------------------------------------\n",
    "    # Public Method to Get Model Builders \n",
    "    # -------------------------------------------------------------------\n",
    "\n",
    "    @staticmethod\n",
    "    def get_all_models():\n",
    "        \"\"\"\n",
    "        Returns a list of all model names available in the ModelFactory.\n",
    "        \"\"\"\n",
    "        return {\n",
    "            \"Efficient_VGG\": ModelFactory.build_efficient_vgg,\n",
    "            \"PaperCNN_Lite\": ModelFactory.build_paper_cnn_lite,\n",
    "            \"SE_AudioCNN\": ModelFactory.build_se_audio_cnn,\n",
    "            \"SeparableResSE_CNN\": ModelFactory.build_separable_res_se_cnn,\n",
    "            \"ResSE_AudioCNN\": ModelFactory.build_res_se_audio_cnn,\n",
    "            'UNet_Audio_Classifier': ModelFactory.build_unet_audio_classifier,\n",
    "        }\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_all_model_names():\n",
    "        \"\"\"\n",
    "        Returns a list of all model names available in the ModelFactory.\n",
    "        \"\"\"\n",
    "        return list(ModelFactory.get_all_models().keys())\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def get_builder_by_name(name):\n",
    "        \"\"\"\n",
    "        Retrieves the model-building function corresponding to a given model name.\n",
    "        This provides a robust, centralized mapping from string names to methods.\n",
    "        \"\"\"\n",
    "\n",
    "        return ModelFactory.get_all_models()[name]\n",
    "\n",
    "    \n",
    "    # -------------------------------------------------------------------\n",
    "    # Helper Building Blocks (Shared across multiple architectures)\n",
    "    # -------------------------------------------------------------------\n",
    "\n",
    "    @staticmethod\n",
    "    def _se_block(input_tensor, ratio=8):\n",
    "        \"\"\"\n",
    "        Squeeze-and-Excitation block. A lightweight channel-wise attention\n",
    "        mechanism to recalibrate feature maps by modeling interdependencies\n",
    "        between channels.\n",
    "        Ref: Hu et al., \"Squeeze-and-Excitation Networks\" (2018)\n",
    "        \"\"\"\n",
    "        channels = input_tensor.shape[-1]\n",
    "        # Squeeze: Global information embedding\n",
    "        se = layers.GlobalAveragePooling2D(name=f'se_squeeze_{input_tensor.name}')(input_tensor)\n",
    "        se = layers.Reshape((1, 1, channels))(se)\n",
    "        # Excitation: Adaptive recalibration\n",
    "        se = layers.Dense(channels // ratio, activation='relu', name=f'se_excite_1_{input_tensor.name}')(se)\n",
    "        se = layers.Dense(channels, activation='sigmoid', name=f'se_excite_2_{input_tensor.name}')(se)\n",
    "        return layers.Multiply(name=f'se_scale_{input_tensor.name}')([input_tensor, se])\n",
    "\n",
    "    # -------------------------------------------------------------------\n",
    "    # Model 1: Efficient VGG-style Baseline\n",
    "    # -------------------------------------------------------------------\n",
    "    @staticmethod\n",
    "    def build_efficient_vgg(input_shape, num_classes):\n",
    "        \"\"\"\n",
    "        A memory-efficient VGG-style model. Starts with a small number of\n",
    "        filters to establish a fast, simple, and memory-safe baseline.\n",
    "        \"\"\"\n",
    "        inputs = layers.Input(shape=input_shape)\n",
    "        x = layers.Conv2D(16, 3, padding='same', use_bias=False)(inputs)\n",
    "        x = layers.BatchNormalization()(x); x = layers.Activation('relu')(x)\n",
    "        x = layers.MaxPooling2D(2)(x); x = ModelFactory._se_block(x)\n",
    "        x = layers.Conv2D(32, 3, padding='same', use_bias=False)(x)\n",
    "        x = layers.BatchNormalization()(x); x = layers.Activation('relu')(x)\n",
    "        x = layers.MaxPooling2D(2)(x); x = ModelFactory._se_block(x)\n",
    "        x = layers.Conv2D(64, 3, padding='same', use_bias=False)(x)\n",
    "        x = layers.BatchNormalization()(x); x = layers.Activation('relu')(x)\n",
    "        x = layers.MaxPooling2D(2)(x); x = ModelFactory._se_block(x)\n",
    "        x = layers.GlobalAveragePooling2D()(x)\n",
    "        x = layers.Dense(128, activation='relu')(x)\n",
    "        x = layers.Dropout(0.5)(x)\n",
    "        outputs = layers.Dense(num_classes, activation='softmax', dtype='float32')(x)\n",
    "        return models.Model(inputs=inputs, outputs=outputs, name='Efficient_VGG')\n",
    "\n",
    "    # -------------------------------------------------------------------\n",
    "    # Model 2: Paper-Inspired Multi-Scale Model\n",
    "    # -------------------------------------------------------------------\n",
    "    @staticmethod\n",
    "    def build_paper_cnn_lite(input_shape, num_classes):\n",
    "        \"\"\"\n",
    "        A memory-optimized interpretation of the paper's multi-scale feature\n",
    "        aggregation concept, using PReLU activation as specified.\n",
    "        \"\"\"\n",
    "        inputs = layers.Input(shape=input_shape)\n",
    "        x1 = layers.Conv2D(16, 3, padding='same', use_bias=False)(inputs)\n",
    "        x1 = layers.BatchNormalization()(x1); x1 = layers.PReLU(shared_axes=[1, 2])(x1)\n",
    "        p1 = layers.MaxPooling2D(2)(x1)\n",
    "        x2 = layers.Conv2D(32, 3, padding='same', use_bias=False)(p1)\n",
    "        x2 = layers.BatchNormalization()(x2); x2 = layers.PReLU(shared_axes=[1, 2])(x2)\n",
    "        p2 = layers.MaxPooling2D(2)(x2)\n",
    "        x3 = layers.Conv2D(64, 3, padding='same', use_bias=False)(p2)\n",
    "        x3 = layers.BatchNormalization()(x3); x3 = layers.PReLU(shared_axes=[1, 2])(x3)\n",
    "        gap1 = layers.GlobalAveragePooling2D()(x1)\n",
    "        gap2 = layers.GlobalAveragePooling2D()(x2)\n",
    "        gap3 = layers.GlobalAveragePooling2D()(x3)\n",
    "        merged_features = layers.Concatenate()([gap1, gap2, gap3])\n",
    "        x = layers.Dense(128, activation='relu')(merged_features)\n",
    "        x = layers.Dropout(0.5)(x)\n",
    "        outputs = layers.Dense(num_classes, activation='softmax', dtype='float32')(x)\n",
    "        return models.Model(inputs=inputs, outputs=outputs, name='PaperCNN_Lite')\n",
    "\n",
    "    # -------------------------------------------------------------------\n",
    "    # Model 3: VGG with Attention\n",
    "    # -------------------------------------------------------------------\n",
    "    @staticmethod\n",
    "    def build_se_audio_cnn(input_shape, num_classes):\n",
    "        \"\"\"\n",
    "        A standard VGG-style architecture enhanced with SE blocks. Tests\n",
    "        the impact of attention on a conventional, non-residual backbone.\n",
    "        \"\"\"\n",
    "        inputs = layers.Input(shape=input_shape)\n",
    "        x = layers.Conv2D(32, 3, padding='same', use_bias=False)(inputs)\n",
    "        x = layers.BatchNormalization()(x); x = layers.Activation('relu')(x)\n",
    "        x = layers.MaxPooling2D(2)(x); x = ModelFactory._se_block(x)\n",
    "        x = layers.Conv2D(64, 3, padding='same', use_bias=False)(x)\n",
    "        x = layers.BatchNormalization()(x); x = layers.Activation('relu')(x)\n",
    "        x = layers.MaxPooling2D(2)(x); x = ModelFactory._se_block(x)\n",
    "        x = layers.Conv2D(128, 3, padding='same', use_bias=False)(x)\n",
    "        x = layers.BatchNormalization()(x); x = layers.Activation('relu')(x)\n",
    "        x = layers.MaxPooling2D(2)(x); x = ModelFactory._se_block(x)\n",
    "        x = layers.GlobalAveragePooling2D()(x)\n",
    "        x = layers.Dense(128, activation='relu')(x)\n",
    "        x = layers.Dropout(0.5)(x)\n",
    "        outputs = layers.Dense(num_classes, activation='softmax', dtype='float32')(x)\n",
    "        return models.Model(inputs=inputs, outputs=outputs, name='SE_AudioCNN')\n",
    "\n",
    "    # -------------------------------------------------------------------\n",
    "    # Model 4: Residual Network with Separable Convolutions\n",
    "    # -------------------------------------------------------------------\n",
    "    @staticmethod\n",
    "    def _separable_res_se_block(input_tensor, filters, stride=1):\n",
    "        \"\"\"Residual block using depthwise separable convolutions for efficiency.\"\"\"\n",
    "        shortcut = input_tensor\n",
    "        x = layers.SeparableConv2D(filters, 3, strides=stride, padding='same', use_bias=False)(input_tensor)\n",
    "        x = layers.BatchNormalization()(x); x = layers.PReLU(shared_axes=[1, 2])(x)\n",
    "        x = layers.SeparableConv2D(filters, 3, padding='same', use_bias=False)(x)\n",
    "        x = layers.BatchNormalization()(x); x = ModelFactory._se_block(x)\n",
    "        if stride > 1 or shortcut.shape[-1] != filters:\n",
    "            shortcut = layers.Conv2D(filters, 1, strides=stride, use_bias=False)(shortcut)\n",
    "            shortcut = layers.BatchNormalization()(shortcut)\n",
    "        x = layers.Add()([shortcut, x]); x = layers.PReLU(shared_axes=[1, 2])(x)\n",
    "        return x\n",
    "\n",
    "    @staticmethod\n",
    "    def build_separable_res_se_cnn(input_shape, num_classes):\n",
    "        \"\"\"A parametrically efficient ResNet-style model.\"\"\"\n",
    "        inputs = layers.Input(shape=input_shape)\n",
    "        x = layers.SeparableConv2D(32, 3, strides=1, padding='same', use_bias=False)(inputs)\n",
    "        x = layers.BatchNormalization()(x); x = layers.PReLU(shared_axes=[1, 2])(x)\n",
    "        x = ModelFactory._separable_res_se_block(x, 64, stride=2)\n",
    "        x = ModelFactory._separable_res_se_block(x, 128, stride=2)\n",
    "        x = ModelFactory._separable_res_se_block(x, 256, stride=2)\n",
    "        x = layers.GlobalAveragePooling2D()(x)\n",
    "        x = layers.Dropout(0.5)(x)\n",
    "        outputs = layers.Dense(num_classes, activation='softmax', dtype='float32')(x)\n",
    "        return models.Model(inputs=inputs, outputs=outputs, name='SeparableResSE_CNN')\n",
    "\n",
    "    # -------------------------------------------------------------------\n",
    "    # Model 5: Residual Network with Standard Convolutions\n",
    "    # -------------------------------------------------------------------\n",
    "    @staticmethod\n",
    "    def _res_se_block(input_tensor, filters, stride=1):\n",
    "        \"\"\"Residual block using standard convolutions.\"\"\"\n",
    "        # ... (implementation is unchanged)\n",
    "        shortcut = input_tensor\n",
    "        x = layers.Conv2D(filters, 3, strides=stride, padding='same', use_bias=False)(input_tensor)\n",
    "        x = layers.BatchNormalization()(x); x = layers.PReLU(shared_axes=[1, 2])(x)\n",
    "        x = layers.Conv2D(filters, 3, padding='same', use_bias=False)(x)\n",
    "        x = layers.BatchNormalization()(x); x = ModelFactory._se_block(x)\n",
    "        if stride > 1 or shortcut.shape[-1] != filters:\n",
    "            shortcut = layers.Conv2D(filters, 1, strides=stride, use_bias=False)(shortcut)\n",
    "            shortcut = layers.BatchNormalization()(shortcut)\n",
    "        x = layers.Add()([shortcut, x]); x = layers.PReLU(shared_axes=[1, 2])(x)\n",
    "        return x\n",
    "\n",
    "    @staticmethod\n",
    "    def build_res_se_audio_cnn(input_shape, num_classes):\n",
    "        \"\"\"\n",
    "        Our most powerful stable architecture, combining ResNet principles\n",
    "        with SE attention and standard convolutions.\n",
    "        \"\"\"\n",
    "        inputs = layers.Input(shape=input_shape)\n",
    "        x = layers.Conv2D(32, 3, strides=1, padding='same', use_bias=False)(inputs)\n",
    "        x = layers.BatchNormalization()(x); x = layers.PReLU(shared_axes=[1, 2])(x)\n",
    "        x = ModelFactory._res_se_block(x, 64, stride=2)\n",
    "        x = ModelFactory._res_se_block(x, 128, stride=2)\n",
    "        x = ModelFactory._res_se_block(x, 256, stride=2)\n",
    "        \n",
    "        # *** BUG FIX: Assign a stable, explicit name to this layer. ***\n",
    "        x = layers.GlobalAveragePooling2D(name=\"gap\")(x)\n",
    "        \n",
    "        x = layers.Dropout(0.5)(x)\n",
    "        outputs = layers.Dense(num_classes, activation='softmax', dtype='float32')(x)\n",
    "        return models.Model(inputs=inputs, outputs=outputs, name='ResSE_AudioCNN')\n",
    "\n",
    "    # -------------------------------------------------------------------\n",
    "    # Model 6: U-Net Inspired Multi-Scale Classifier (NEW!)\n",
    "    # -------------------------------------------------------------------\n",
    "    @staticmethod\n",
    "    def _unet_encoder_block(input_tensor, filters, pool=True):\n",
    "        \"\"\"A block for the U-Net's encoder path (down-sampling).\"\"\"\n",
    "        x = layers.Conv2D(filters, 3, padding='same', use_bias=False)(input_tensor)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.PReLU(shared_axes=[1, 2])(x)\n",
    "        x = layers.Conv2D(filters, 3, padding='same', use_bias=False)(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.PReLU(shared_axes=[1, 2])(x)\n",
    "        \n",
    "        # The output before pooling is the skip connection\n",
    "        skip_connection = x\n",
    "        \n",
    "        if pool:\n",
    "            pool_output = layers.MaxPooling2D(2)(x)\n",
    "            return pool_output, skip_connection\n",
    "        else:\n",
    "            return x, skip_connection\n",
    "\n",
    "    @staticmethod\n",
    "    def _unet_decoder_block(input_tensor, skip_connection, filters):\n",
    "        \"\"\"A block for the U-Net's decoder path (up-sampling).\"\"\"\n",
    "        # Upsample the input tensor\n",
    "        x = layers.Conv2DTranspose(filters, 2, strides=2, padding='same')(input_tensor)\n",
    "        \n",
    "        # Concatenate with the skip connection from the encoder\n",
    "        x = layers.Concatenate()([x, skip_connection])\n",
    "        \n",
    "        # Standard convolutions\n",
    "        x = layers.Conv2D(filters, 3, padding='same', use_bias=False)(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.PReLU(shared_axes=[1, 2])(x)\n",
    "        x = layers.Conv2D(filters, 3, padding='same', use_bias=False)(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.PReLU(shared_axes=[1, 2])(x)\n",
    "        return x\n",
    "\n",
    "    @staticmethod\n",
    "    def build_unet_audio_classifier(input_shape, num_classes):\n",
    "        \"\"\"\n",
    "        A U-Net inspired model adapted for classification. It uses the U-Net's\n",
    "        bottleneck as a rich feature extractor for the final classification task,\n",
    "        embodying the paper's multi-scale philosophy.\n",
    "        \"\"\"\n",
    "        inputs = layers.Input(shape=input_shape)\n",
    "        \n",
    "        # Encoder Path\n",
    "        p1, s1 = ModelFactory._unet_encoder_block(inputs, 32)\n",
    "        p2, s2 = ModelFactory._unet_encoder_block(p1, 64)\n",
    "        p3, s3 = ModelFactory._unet_encoder_block(p2, 128)\n",
    "        \n",
    "        # Bottleneck\n",
    "        # The bottleneck also returns a \"skip connection\" but we won't use it for the decoder\n",
    "        bottleneck, _ = ModelFactory._unet_encoder_block(p3, 256, pool=False)\n",
    "        \n",
    "        # We will use the bottleneck for classification\n",
    "        x = layers.GlobalAveragePooling2D()(bottleneck)\n",
    "        x = layers.Dropout(0.5)(x)\n",
    "        outputs = layers.Dense(num_classes, activation='softmax', dtype='float32')(x)\n",
    "        \n",
    "        # We can still build the decoder path if we wanted to do multi-task learning,\n",
    "        # but for this experiment, we only need the classification output.\n",
    "        \n",
    "        return models.Model(inputs=inputs, outputs=outputs, name='UNet_Audio_Classifier')\n",
    "\n",
    "\n",
    "print(\"‚úÖ ModelFactory defined with 5 candidate architectures for the final comparative analysis.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a6c0fb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pre-processed data...\n",
      "‚úÖ Data successfully loaded and prepared.\n",
      "\n",
      "Configuring JIT data pipelines (without caching)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1753526071.643047    6655 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 10162 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4070, pci bus id: 0000:01:00.0, compute capability: 8.9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "TRAINING ARCHITECTURE: 'Efficient_VGG'\n",
      "================================================================================\n",
      "üöÄ Starting training for model: UNet_Audio_Classifier...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1753526076.014569    6880 service.cc:152] XLA service 0x7cb4a8023ce0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1753526076.014588    6880 service.cc:160]   StreamExecutor device (0): NVIDIA GeForce RTX 4070, Compute Capability 8.9\n",
      "2025-07-26 12:34:36.114469: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "I0000 00:00:1753526076.718186    6880 cuda_dnn.cc:529] Loaded cuDNN version 90300\n",
      "I0000 00:00:1753526085.645809    6880 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01/50 | Time: 1753526101.15s | Loss: 1.4966 | Acc: 0.4624 | Val Loss: 4.6475 | Val Acc: 0.1040 | LR: 1.0e-03 ‚úÖ\n",
      "Epoch 02/50 | Time: 1753526105.55s | Loss: 1.0511 | Acc: 0.6225 | Val Loss: 4.6654 | Val Acc: 0.1210 | LR: 1.0e-03 ‚úÖ\n",
      "Epoch 03/50 | Time: 1753526109.82s | Loss: 0.8885 | Acc: 0.6922 | Val Loss: 2.9475 | Val Acc: 0.2350 | LR: 1.0e-03 ‚úÖ\n",
      "Epoch 04/50 | Time: 1753526114.09s | Loss: 0.6908 | Acc: 0.7658 | Val Loss: 1.7448 | Val Acc: 0.4390 | LR: 1.0e-03 ‚úÖ\n",
      "Epoch 05/50 | Time: 1753526118.35s | Loss: 0.5654 | Acc: 0.8174 | Val Loss: 3.0793 | Val Acc: 0.3875 | LR: 1.0e-03\n",
      "Epoch 06/50 | Time: 1753526122.54s | Loss: 0.4818 | Acc: 0.8501 | Val Loss: 1.3272 | Val Acc: 0.5490 | LR: 1.0e-03 ‚úÖ\n",
      "Epoch 07/50 | Time: 1753526126.82s | Loss: 0.4284 | Acc: 0.8578 | Val Loss: 2.3462 | Val Acc: 0.5065 | LR: 1.0e-03\n",
      "Epoch 08/50 | Time: 1753526131.03s | Loss: 0.3467 | Acc: 0.8890 | Val Loss: 1.9297 | Val Acc: 0.5150 | LR: 1.0e-03\n",
      "Epoch 09/50 | Time: 1753526135.23s | Loss: 0.2897 | Acc: 0.9097 | Val Loss: 1.6867 | Val Acc: 0.6170 | LR: 1.0e-03 ‚úÖ\n",
      "Epoch 10/50 | Time: 1753526139.52s | Loss: 0.2535 | Acc: 0.9195 | Val Loss: 4.7069 | Val Acc: 0.3900 | LR: 1.0e-03\n",
      "Epoch 11/50 | Time: 1753526143.71s | Loss: 0.2115 | Acc: 0.9332 | Val Loss: 2.2353 | Val Acc: 0.5550 | LR: 1.0e-03\n",
      "\n",
      "Epoch 11: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "Epoch 12/50 | Time: 1753526147.92s | Loss: 0.1181 | Acc: 0.9691 | Val Loss: 0.8984 | Val Acc: 0.7585 | LR: 2.0e-04 ‚úÖ\n",
      "Epoch 13/50 | Time: 1753526152.28s | Loss: 0.0875 | Acc: 0.9790 | Val Loss: 0.7396 | Val Acc: 0.7795 | LR: 2.0e-04 ‚úÖ\n",
      "Epoch 14/50 | Time: 1753526156.57s | Loss: 0.0804 | Acc: 0.9800 | Val Loss: 0.7036 | Val Acc: 0.8030 | LR: 2.0e-04 ‚úÖ\n",
      "Epoch 15/50 | Time: 1753526160.91s | Loss: 0.0649 | Acc: 0.9866 | Val Loss: 1.0138 | Val Acc: 0.7260 | LR: 2.0e-04\n",
      "Epoch 16/50 | Time: 1753526165.12s | Loss: 0.0627 | Acc: 0.9871 | Val Loss: 0.7621 | Val Acc: 0.8015 | LR: 2.0e-04\n",
      "Epoch 17/50 | Time: 1753526169.33s | Loss: 0.0591 | Acc: 0.9868 | Val Loss: 0.8532 | Val Acc: 0.7700 | LR: 2.0e-04\n",
      "Epoch 18/50 | Time: 1753526173.59s | Loss: 0.0530 | Acc: 0.9885 | Val Loss: 0.9031 | Val Acc: 0.7755 | LR: 2.0e-04\n",
      "Epoch 19/50 | Time: 1753526177.80s | Loss: 0.0518 | Acc: 0.9898 | Val Loss: 0.7521 | Val Acc: 0.8015 | LR: 2.0e-04\n",
      "\n",
      "Epoch 19: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
      "Epoch 20/50 | Time: 1753526181.99s | Loss: 0.0403 | Acc: 0.9928 | Val Loss: 0.6255 | Val Acc: 0.8245 | LR: 4.0e-05 ‚úÖ\n",
      "Epoch 21/50 | Time: 1753526186.29s | Loss: 0.0393 | Acc: 0.9932 | Val Loss: 0.6292 | Val Acc: 0.8210 | LR: 4.0e-05\n",
      "Epoch 22/50 | Time: 1753526190.53s | Loss: 0.0360 | Acc: 0.9940 | Val Loss: 0.6509 | Val Acc: 0.8175 | LR: 4.0e-05\n",
      "Epoch 23/50 | Time: 1753526194.74s | Loss: 0.0366 | Acc: 0.9938 | Val Loss: 0.7088 | Val Acc: 0.8065 | LR: 4.0e-05\n",
      "Epoch 24/50 | Time: 1753526198.94s | Loss: 0.0338 | Acc: 0.9943 | Val Loss: 0.7210 | Val Acc: 0.8110 | LR: 4.0e-05\n",
      "Epoch 25/50 | Time: 1753526203.18s | Loss: 0.0336 | Acc: 0.9952 | Val Loss: 0.6955 | Val Acc: 0.8240 | LR: 4.0e-05\n",
      "\n",
      "Epoch 25: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.\n",
      "Epoch 26/50 | Time: 1753526207.40s | Loss: 0.0320 | Acc: 0.9953 | Val Loss: 0.6528 | Val Acc: 0.8305 | LR: 8.0e-06 ‚úÖ\n",
      "Epoch 27/50 | Time: 1753526211.73s | Loss: 0.0338 | Acc: 0.9933 | Val Loss: 0.6303 | Val Acc: 0.8330 | LR: 8.0e-06 ‚úÖ\n",
      "Epoch 28/50 | Time: 1753526216.05s | Loss: 0.0312 | Acc: 0.9963 | Val Loss: 0.6200 | Val Acc: 0.8375 | LR: 8.0e-06 ‚úÖ\n",
      "Epoch 29/50 | Time: 1753526220.35s | Loss: 0.0329 | Acc: 0.9943 | Val Loss: 0.6269 | Val Acc: 0.8330 | LR: 8.0e-06\n",
      "Epoch 30/50 | Time: 1753526224.58s | Loss: 0.0307 | Acc: 0.9947 | Val Loss: 0.6467 | Val Acc: 0.8335 | LR: 8.0e-06\n",
      "Epoch 31/50 | Time: 1753526228.83s | Loss: 0.0307 | Acc: 0.9968 | Val Loss: 0.6327 | Val Acc: 0.8310 | LR: 8.0e-06\n",
      "Epoch 32/50 | Time: 1753526233.04s | Loss: 0.0293 | Acc: 0.9970 | Val Loss: 0.6257 | Val Acc: 0.8295 | LR: 8.0e-06\n",
      "Epoch 33/50 | Time: 1753526237.27s | Loss: 0.0310 | Acc: 0.9952 | Val Loss: 0.6408 | Val Acc: 0.8290 | LR: 8.0e-06\n",
      "\n",
      "Epoch 33: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.\n",
      "Epoch 34/50 | Time: 1753526241.53s | Loss: 0.0302 | Acc: 0.9953 | Val Loss: 0.6271 | Val Acc: 0.8305 | LR: 1.6e-06\n",
      "Epoch 35/50 | Time: 1753526245.74s | Loss: 0.0314 | Acc: 0.9963 | Val Loss: 0.6248 | Val Acc: 0.8350 | LR: 1.6e-06\n",
      "Epoch 36/50 | Time: 1753526249.98s | Loss: 0.0325 | Acc: 0.9947 | Val Loss: 0.6243 | Val Acc: 0.8340 | LR: 1.6e-06\n",
      "Epoch 37/50 | Time: 1753526254.19s | Loss: 0.0315 | Acc: 0.9955 | Val Loss: 0.6231 | Val Acc: 0.8340 | LR: 1.6e-06\n",
      "Epoch 38/50 | Time: 1753526258.42s | Loss: 0.0324 | Acc: 0.9952 | Val Loss: 0.6240 | Val Acc: 0.8330 | LR: 1.6e-06\n",
      "\n",
      "Epoch 38: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.\n",
      "Epoch 39/50 | Time: 1753526262.64s | Loss: 0.0291 | Acc: 0.9967 | Val Loss: 0.6235 | Val Acc: 0.8345 | LR: 3.2e-07\n",
      "Epoch 40/50 | Time: 1753526266.90s | Loss: 0.0290 | Acc: 0.9970 | Val Loss: 0.6249 | Val Acc: 0.8340 | LR: 3.2e-07\n",
      "Epoch 41/50 | Time: 1753526271.12s | Loss: 0.0297 | Acc: 0.9958 | Val Loss: 0.6257 | Val Acc: 0.8335 | LR: 3.2e-07\n",
      "Epoch 42/50 | Time: 1753526275.33s | Loss: 0.0279 | Acc: 0.9963 | Val Loss: 0.6240 | Val Acc: 0.8345 | LR: 3.2e-07\n",
      "Epoch 43/50 | Time: 1753526279.59s | Loss: 0.0328 | Acc: 0.9957 | Val Loss: 0.6251 | Val Acc: 0.8340 | LR: 3.2e-07\n",
      "\n",
      "Epoch 43: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-08.\n",
      "üèÅ Finished training. Best Validation Accuracy: 0.8375\n",
      "\n",
      "================================================================================\n",
      "TRAINING ARCHITECTURE: 'PaperCNN_Lite'\n",
      "================================================================================\n",
      "üöÄ Starting training for model: UNet_Audio_Classifier...\n",
      "Epoch 01/50 | Time: 1753526295.13s | Loss: 1.5152 | Acc: 0.4699 | Val Loss: 4.2858 | Val Acc: 0.1585 | LR: 1.0e-03 ‚úÖ\n",
      "Epoch 02/50 | Time: 1753526299.51s | Loss: 1.0763 | Acc: 0.6292 | Val Loss: 4.3318 | Val Acc: 0.1000 | LR: 1.0e-03\n",
      "Epoch 03/50 | Time: 1753526303.72s | Loss: 0.8642 | Acc: 0.7040 | Val Loss: 4.0850 | Val Acc: 0.1545 | LR: 1.0e-03\n",
      "Epoch 04/50 | Time: 1753526307.97s | Loss: 0.7007 | Acc: 0.7694 | Val Loss: 2.6383 | Val Acc: 0.4465 | LR: 1.0e-03 ‚úÖ\n",
      "Epoch 05/50 | Time: 1753526312.27s | Loss: 0.6002 | Acc: 0.7972 | Val Loss: 2.2578 | Val Acc: 0.4745 | LR: 1.0e-03 ‚úÖ\n",
      "Epoch 06/50 | Time: 1753526316.65s | Loss: 0.5162 | Acc: 0.8257 | Val Loss: 1.4537 | Val Acc: 0.6455 | LR: 1.0e-03 ‚úÖ\n",
      "Epoch 07/50 | Time: 1753526321.06s | Loss: 0.4379 | Acc: 0.8581 | Val Loss: 1.6822 | Val Acc: 0.4905 | LR: 1.0e-03\n",
      "Epoch 08/50 | Time: 1753526325.32s | Loss: 0.3828 | Acc: 0.8716 | Val Loss: 1.3918 | Val Acc: 0.6000 | LR: 1.0e-03\n",
      "Epoch 09/50 | Time: 1753526329.58s | Loss: 0.3141 | Acc: 0.8965 | Val Loss: 2.2127 | Val Acc: 0.5745 | LR: 1.0e-03\n",
      "Epoch 10/50 | Time: 1753526333.80s | Loss: 0.2915 | Acc: 0.9038 | Val Loss: 9.4213 | Val Acc: 0.1840 | LR: 1.0e-03\n",
      "Epoch 11/50 | Time: 1753526338.06s | Loss: 0.2313 | Acc: 0.9239 | Val Loss: 3.0261 | Val Acc: 0.5010 | LR: 1.0e-03\n",
      "Epoch 12/50 | Time: 1753526342.29s | Loss: 0.1982 | Acc: 0.9347 | Val Loss: 2.0268 | Val Acc: 0.5790 | LR: 1.0e-03\n",
      "Epoch 13/50 | Time: 1753526346.52s | Loss: 0.1678 | Acc: 0.9481 | Val Loss: 3.2282 | Val Acc: 0.5690 | LR: 1.0e-03\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "Epoch 14/50 | Time: 1753526350.79s | Loss: 0.0988 | Acc: 0.9730 | Val Loss: 0.8525 | Val Acc: 0.7815 | LR: 2.0e-04 ‚úÖ\n",
      "Epoch 15/50 | Time: 1753526355.11s | Loss: 0.0690 | Acc: 0.9856 | Val Loss: 0.8145 | Val Acc: 0.7705 | LR: 2.0e-04\n",
      "Epoch 16/50 | Time: 1753526359.34s | Loss: 0.0548 | Acc: 0.9898 | Val Loss: 0.9083 | Val Acc: 0.7495 | LR: 2.0e-04\n",
      "Epoch 17/50 | Time: 1753526363.64s | Loss: 0.0495 | Acc: 0.9905 | Val Loss: 1.0170 | Val Acc: 0.7660 | LR: 2.0e-04\n",
      "Epoch 18/50 | Time: 1753526367.88s | Loss: 0.0441 | Acc: 0.9920 | Val Loss: 0.9851 | Val Acc: 0.7320 | LR: 2.0e-04\n",
      "Epoch 19/50 | Time: 1753526372.11s | Loss: 0.0443 | Acc: 0.9915 | Val Loss: 0.8171 | Val Acc: 0.7830 | LR: 2.0e-04 ‚úÖ\n",
      "Epoch 20/50 | Time: 1753526376.48s | Loss: 0.0373 | Acc: 0.9940 | Val Loss: 1.0792 | Val Acc: 0.7600 | LR: 2.0e-04\n",
      "\n",
      "Epoch 20: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
      "Epoch 21/50 | Time: 1753526380.76s | Loss: 0.0383 | Acc: 0.9923 | Val Loss: 0.7482 | Val Acc: 0.8175 | LR: 4.0e-05 ‚úÖ\n",
      "Epoch 22/50 | Time: 1753526385.20s | Loss: 0.0330 | Acc: 0.9950 | Val Loss: 0.6730 | Val Acc: 0.8265 | LR: 4.0e-05 ‚úÖ\n",
      "Epoch 23/50 | Time: 1753526389.56s | Loss: 0.0320 | Acc: 0.9947 | Val Loss: 0.7058 | Val Acc: 0.8235 | LR: 4.0e-05\n",
      "Epoch 24/50 | Time: 1753526393.81s | Loss: 0.0329 | Acc: 0.9940 | Val Loss: 0.6562 | Val Acc: 0.8310 | LR: 4.0e-05 ‚úÖ\n",
      "Epoch 25/50 | Time: 1753526398.13s | Loss: 0.0334 | Acc: 0.9952 | Val Loss: 0.7583 | Val Acc: 0.8225 | LR: 4.0e-05\n",
      "Epoch 26/50 | Time: 1753526402.41s | Loss: 0.0270 | Acc: 0.9968 | Val Loss: 0.7103 | Val Acc: 0.8260 | LR: 4.0e-05\n",
      "Epoch 27/50 | Time: 1753526406.67s | Loss: 0.0293 | Acc: 0.9963 | Val Loss: 0.7050 | Val Acc: 0.8190 | LR: 4.0e-05\n",
      "Epoch 28/50 | Time: 1753526410.94s | Loss: 0.0267 | Acc: 0.9958 | Val Loss: 0.6900 | Val Acc: 0.8245 | LR: 4.0e-05\n",
      "Epoch 29/50 | Time: 1753526415.15s | Loss: 0.0255 | Acc: 0.9968 | Val Loss: 0.6749 | Val Acc: 0.8260 | LR: 4.0e-05\n",
      "\n",
      "Epoch 29: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.\n",
      "Epoch 30/50 | Time: 1753526419.44s | Loss: 0.0261 | Acc: 0.9963 | Val Loss: 0.6790 | Val Acc: 0.8275 | LR: 8.0e-06\n",
      "Epoch 31/50 | Time: 1753526423.71s | Loss: 0.0254 | Acc: 0.9958 | Val Loss: 0.6927 | Val Acc: 0.8260 | LR: 8.0e-06\n",
      "Epoch 32/50 | Time: 1753526427.92s | Loss: 0.0272 | Acc: 0.9965 | Val Loss: 0.6737 | Val Acc: 0.8290 | LR: 8.0e-06\n",
      "Epoch 33/50 | Time: 1753526432.20s | Loss: 0.0259 | Acc: 0.9963 | Val Loss: 0.6841 | Val Acc: 0.8295 | LR: 8.0e-06\n",
      "Epoch 34/50 | Time: 1753526436.47s | Loss: 0.0244 | Acc: 0.9967 | Val Loss: 0.6896 | Val Acc: 0.8265 | LR: 8.0e-06\n",
      "\n",
      "Epoch 34: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.\n",
      "Epoch 35/50 | Time: 1753526440.71s | Loss: 0.0258 | Acc: 0.9965 | Val Loss: 0.6880 | Val Acc: 0.8290 | LR: 1.6e-06\n",
      "Epoch 36/50 | Time: 1753526445.05s | Loss: 0.0270 | Acc: 0.9950 | Val Loss: 0.6802 | Val Acc: 0.8280 | LR: 1.6e-06\n",
      "Epoch 37/50 | Time: 1753526449.34s | Loss: 0.0230 | Acc: 0.9958 | Val Loss: 0.6783 | Val Acc: 0.8280 | LR: 1.6e-06\n",
      "Epoch 38/50 | Time: 1753526453.56s | Loss: 0.0251 | Acc: 0.9965 | Val Loss: 0.6781 | Val Acc: 0.8270 | LR: 1.6e-06\n",
      "Epoch 39/50 | Time: 1753526457.83s | Loss: 0.0278 | Acc: 0.9955 | Val Loss: 0.6798 | Val Acc: 0.8280 | LR: 1.6e-06\n",
      "\n",
      "Epoch 39: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.\n",
      "üèÅ Finished training. Best Validation Accuracy: 0.8310\n",
      "\n",
      "================================================================================\n",
      "TRAINING ARCHITECTURE: 'SE_AudioCNN'\n",
      "================================================================================\n",
      "üöÄ Starting training for model: UNet_Audio_Classifier...\n",
      "Epoch 01/50 | Time: 1753526473.41s | Loss: 1.5474 | Acc: 0.4472 | Val Loss: 3.8726 | Val Acc: 0.1190 | LR: 1.0e-03 ‚úÖ\n",
      "Epoch 02/50 | Time: 1753526477.83s | Loss: 1.0680 | Acc: 0.6250 | Val Loss: 4.5440 | Val Acc: 0.1380 | LR: 1.0e-03 ‚úÖ\n",
      "Epoch 03/50 | Time: 1753526482.19s | Loss: 0.8607 | Acc: 0.7072 | Val Loss: 2.8569 | Val Acc: 0.3410 | LR: 1.0e-03 ‚úÖ\n",
      "Epoch 04/50 | Time: 1753526486.64s | Loss: 0.7136 | Acc: 0.7563 | Val Loss: 2.3189 | Val Acc: 0.4080 | LR: 1.0e-03 ‚úÖ\n",
      "Epoch 05/50 | Time: 1753526491.01s | Loss: 0.5928 | Acc: 0.8028 | Val Loss: 1.2558 | Val Acc: 0.5980 | LR: 1.0e-03 ‚úÖ\n",
      "Epoch 06/50 | Time: 1753526495.33s | Loss: 0.5123 | Acc: 0.8369 | Val Loss: 1.3552 | Val Acc: 0.6120 | LR: 1.0e-03 ‚úÖ\n",
      "Epoch 07/50 | Time: 1753526499.68s | Loss: 0.4078 | Acc: 0.8673 | Val Loss: 2.1637 | Val Acc: 0.6320 | LR: 1.0e-03 ‚úÖ\n",
      "Epoch 08/50 | Time: 1753526504.09s | Loss: 0.3542 | Acc: 0.8828 | Val Loss: 2.5117 | Val Acc: 0.4750 | LR: 1.0e-03\n",
      "Epoch 09/50 | Time: 1753526508.34s | Loss: 0.3083 | Acc: 0.9022 | Val Loss: 3.7608 | Val Acc: 0.4280 | LR: 1.0e-03\n",
      "Epoch 10/50 | Time: 1753526512.63s | Loss: 0.2554 | Acc: 0.9179 | Val Loss: 1.8251 | Val Acc: 0.5420 | LR: 1.0e-03\n",
      "\n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "Epoch 11/50 | Time: 1753526516.91s | Loss: 0.1527 | Acc: 0.9558 | Val Loss: 0.8030 | Val Acc: 0.7450 | LR: 2.0e-04 ‚úÖ\n",
      "Epoch 12/50 | Time: 1753526521.29s | Loss: 0.1100 | Acc: 0.9726 | Val Loss: 0.7884 | Val Acc: 0.7490 | LR: 2.0e-04 ‚úÖ\n",
      "Epoch 13/50 | Time: 1753526525.63s | Loss: 0.1023 | Acc: 0.9738 | Val Loss: 0.8700 | Val Acc: 0.7400 | LR: 2.0e-04\n",
      "Epoch 14/50 | Time: 1753526529.91s | Loss: 0.0888 | Acc: 0.9791 | Val Loss: 0.8073 | Val Acc: 0.7715 | LR: 2.0e-04 ‚úÖ\n",
      "Epoch 15/50 | Time: 1753526534.24s | Loss: 0.0753 | Acc: 0.9841 | Val Loss: 0.9071 | Val Acc: 0.7560 | LR: 2.0e-04\n",
      "Epoch 16/50 | Time: 1753526538.55s | Loss: 0.0785 | Acc: 0.9806 | Val Loss: 1.2590 | Val Acc: 0.6910 | LR: 2.0e-04\n",
      "Epoch 17/50 | Time: 1753526542.84s | Loss: 0.0727 | Acc: 0.9818 | Val Loss: 1.3724 | Val Acc: 0.7170 | LR: 2.0e-04\n",
      "\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
      "Epoch 18/50 | Time: 1753526547.06s | Loss: 0.0546 | Acc: 0.9902 | Val Loss: 0.6933 | Val Acc: 0.8095 | LR: 4.0e-05 ‚úÖ\n",
      "Epoch 19/50 | Time: 1753526551.49s | Loss: 0.0528 | Acc: 0.9893 | Val Loss: 0.6759 | Val Acc: 0.8035 | LR: 4.0e-05\n",
      "Epoch 20/50 | Time: 1753526555.73s | Loss: 0.0516 | Acc: 0.9918 | Val Loss: 0.7107 | Val Acc: 0.8010 | LR: 4.0e-05\n",
      "Epoch 21/50 | Time: 1753526560.02s | Loss: 0.0500 | Acc: 0.9917 | Val Loss: 0.7071 | Val Acc: 0.8055 | LR: 4.0e-05\n",
      "Epoch 22/50 | Time: 1753526564.36s | Loss: 0.0505 | Acc: 0.9913 | Val Loss: 0.7193 | Val Acc: 0.8050 | LR: 4.0e-05\n",
      "Epoch 23/50 | Time: 1753526568.64s | Loss: 0.0459 | Acc: 0.9918 | Val Loss: 0.7035 | Val Acc: 0.8075 | LR: 4.0e-05\n",
      "Epoch 24/50 | Time: 1753526572.94s | Loss: 0.0487 | Acc: 0.9910 | Val Loss: 0.6864 | Val Acc: 0.8095 | LR: 4.0e-05\n",
      "\n",
      "Epoch 24: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.\n",
      "Epoch 25/50 | Time: 1753526577.22s | Loss: 0.0435 | Acc: 0.9920 | Val Loss: 0.6683 | Val Acc: 0.8135 | LR: 8.0e-06 ‚úÖ\n",
      "Epoch 26/50 | Time: 1753526581.57s | Loss: 0.0457 | Acc: 0.9912 | Val Loss: 0.6689 | Val Acc: 0.8160 | LR: 8.0e-06 ‚úÖ\n",
      "Epoch 27/50 | Time: 1753526586.03s | Loss: 0.0449 | Acc: 0.9918 | Val Loss: 0.6774 | Val Acc: 0.8100 | LR: 8.0e-06\n",
      "Epoch 28/50 | Time: 1753526590.29s | Loss: 0.0435 | Acc: 0.9932 | Val Loss: 0.6932 | Val Acc: 0.8145 | LR: 8.0e-06\n",
      "Epoch 29/50 | Time: 1753526594.57s | Loss: 0.0440 | Acc: 0.9915 | Val Loss: 0.6803 | Val Acc: 0.8160 | LR: 8.0e-06\n",
      "Epoch 30/50 | Time: 1753526598.84s | Loss: 0.0445 | Acc: 0.9915 | Val Loss: 0.6867 | Val Acc: 0.8155 | LR: 8.0e-06\n",
      "\n",
      "Epoch 30: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.\n",
      "Epoch 31/50 | Time: 1753526603.11s | Loss: 0.0390 | Acc: 0.9930 | Val Loss: 0.6821 | Val Acc: 0.8105 | LR: 1.6e-06\n",
      "Epoch 32/50 | Time: 1753526607.42s | Loss: 0.0440 | Acc: 0.9928 | Val Loss: 0.6804 | Val Acc: 0.8125 | LR: 1.6e-06\n",
      "Epoch 33/50 | Time: 1753526611.64s | Loss: 0.0413 | Acc: 0.9940 | Val Loss: 0.6793 | Val Acc: 0.8130 | LR: 1.6e-06\n",
      "Epoch 34/50 | Time: 1753526615.93s | Loss: 0.0433 | Acc: 0.9913 | Val Loss: 0.6822 | Val Acc: 0.8120 | LR: 1.6e-06\n",
      "Epoch 35/50 | Time: 1753526620.21s | Loss: 0.0451 | Acc: 0.9928 | Val Loss: 0.6809 | Val Acc: 0.8125 | LR: 1.6e-06\n",
      "\n",
      "Epoch 35: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.\n",
      "Epoch 36/50 | Time: 1753526624.52s | Loss: 0.0432 | Acc: 0.9930 | Val Loss: 0.6791 | Val Acc: 0.8140 | LR: 3.2e-07\n",
      "Epoch 37/50 | Time: 1753526628.77s | Loss: 0.0413 | Acc: 0.9937 | Val Loss: 0.6790 | Val Acc: 0.8150 | LR: 3.2e-07\n",
      "Epoch 38/50 | Time: 1753526633.05s | Loss: 0.0436 | Acc: 0.9935 | Val Loss: 0.6787 | Val Acc: 0.8140 | LR: 3.2e-07\n",
      "Epoch 39/50 | Time: 1753526637.32s | Loss: 0.0395 | Acc: 0.9943 | Val Loss: 0.6796 | Val Acc: 0.8125 | LR: 3.2e-07\n",
      "Epoch 40/50 | Time: 1753526641.56s | Loss: 0.0401 | Acc: 0.9938 | Val Loss: 0.6795 | Val Acc: 0.8115 | LR: 3.2e-07\n",
      "\n",
      "Epoch 40: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-08.\n",
      "Epoch 41/50 | Time: 1753526645.87s | Loss: 0.0417 | Acc: 0.9945 | Val Loss: 0.6789 | Val Acc: 0.8125 | LR: 6.4e-08\n",
      "üèÅ Finished training. Best Validation Accuracy: 0.8160\n",
      "\n",
      "================================================================================\n",
      "TRAINING ARCHITECTURE: 'SeparableResSE_CNN'\n",
      "================================================================================\n",
      "üöÄ Starting training for model: UNet_Audio_Classifier...\n",
      "Epoch 01/50 | Time: 1753526662.22s | Loss: 1.5677 | Acc: 0.4404 | Val Loss: 3.8144 | Val Acc: 0.1155 | LR: 1.0e-03 ‚úÖ\n",
      "Epoch 02/50 | Time: 1753526666.60s | Loss: 1.1312 | Acc: 0.6040 | Val Loss: 4.4469 | Val Acc: 0.1855 | LR: 1.0e-03 ‚úÖ\n",
      "Epoch 03/50 | Time: 1753526671.07s | Loss: 0.9070 | Acc: 0.6860 | Val Loss: 2.8852 | Val Acc: 0.3610 | LR: 1.0e-03 ‚úÖ\n",
      "Epoch 04/50 | Time: 1753526675.44s | Loss: 0.7585 | Acc: 0.7449 | Val Loss: 1.7832 | Val Acc: 0.5230 | LR: 1.0e-03 ‚úÖ\n",
      "Epoch 05/50 | Time: 1753526679.94s | Loss: 0.6375 | Acc: 0.7881 | Val Loss: 2.1904 | Val Acc: 0.5220 | LR: 1.0e-03\n",
      "Epoch 06/50 | Time: 1753526684.18s | Loss: 0.5134 | Acc: 0.8312 | Val Loss: 3.8715 | Val Acc: 0.4125 | LR: 1.0e-03\n",
      "Epoch 07/50 | Time: 1753526688.53s | Loss: 0.4490 | Acc: 0.8486 | Val Loss: 3.5287 | Val Acc: 0.3725 | LR: 1.0e-03\n",
      "Epoch 08/50 | Time: 1753526692.87s | Loss: 0.3841 | Acc: 0.8726 | Val Loss: 2.4990 | Val Acc: 0.5395 | LR: 1.0e-03 ‚úÖ\n",
      "Epoch 09/50 | Time: 1753526697.20s | Loss: 0.3287 | Acc: 0.8930 | Val Loss: 2.6418 | Val Acc: 0.4615 | LR: 1.0e-03\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "Epoch 10/50 | Time: 1753526701.49s | Loss: 0.2107 | Acc: 0.9357 | Val Loss: 1.0636 | Val Acc: 0.6770 | LR: 2.0e-04 ‚úÖ\n",
      "Epoch 11/50 | Time: 1753526705.87s | Loss: 0.1653 | Acc: 0.9541 | Val Loss: 0.8613 | Val Acc: 0.7725 | LR: 2.0e-04 ‚úÖ\n",
      "Epoch 12/50 | Time: 1753526710.31s | Loss: 0.1514 | Acc: 0.9584 | Val Loss: 0.9212 | Val Acc: 0.7455 | LR: 2.0e-04\n",
      "Epoch 13/50 | Time: 1753526714.59s | Loss: 0.1368 | Acc: 0.9619 | Val Loss: 0.7297 | Val Acc: 0.7920 | LR: 2.0e-04 ‚úÖ\n",
      "Epoch 14/50 | Time: 1753526718.96s | Loss: 0.1187 | Acc: 0.9731 | Val Loss: 0.9177 | Val Acc: 0.7610 | LR: 2.0e-04\n",
      "Epoch 15/50 | Time: 1753526723.24s | Loss: 0.1095 | Acc: 0.9741 | Val Loss: 0.8417 | Val Acc: 0.7685 | LR: 2.0e-04\n",
      "Epoch 16/50 | Time: 1753526727.53s | Loss: 0.0939 | Acc: 0.9781 | Val Loss: 0.9466 | Val Acc: 0.7475 | LR: 2.0e-04\n",
      "Epoch 17/50 | Time: 1753526731.85s | Loss: 0.0952 | Acc: 0.9775 | Val Loss: 0.9642 | Val Acc: 0.7460 | LR: 2.0e-04\n",
      "Epoch 18/50 | Time: 1753526736.09s | Loss: 0.0876 | Acc: 0.9773 | Val Loss: 0.8690 | Val Acc: 0.7635 | LR: 2.0e-04\n",
      "\n",
      "Epoch 18: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
      "Epoch 19/50 | Time: 1753526740.43s | Loss: 0.0653 | Acc: 0.9868 | Val Loss: 0.6344 | Val Acc: 0.8205 | LR: 4.0e-05 ‚úÖ\n",
      "Epoch 20/50 | Time: 1753526744.87s | Loss: 0.0622 | Acc: 0.9878 | Val Loss: 0.7106 | Val Acc: 0.8060 | LR: 4.0e-05\n",
      "Epoch 21/50 | Time: 1753526749.16s | Loss: 0.0539 | Acc: 0.9917 | Val Loss: 0.6700 | Val Acc: 0.8095 | LR: 4.0e-05\n",
      "Epoch 22/50 | Time: 1753526753.46s | Loss: 0.0580 | Acc: 0.9903 | Val Loss: 0.7259 | Val Acc: 0.8050 | LR: 4.0e-05\n",
      "Epoch 23/50 | Time: 1753526757.75s | Loss: 0.0557 | Acc: 0.9881 | Val Loss: 0.6762 | Val Acc: 0.8060 | LR: 4.0e-05\n",
      "Epoch 24/50 | Time: 1753526762.06s | Loss: 0.0549 | Acc: 0.9883 | Val Loss: 0.7168 | Val Acc: 0.8040 | LR: 4.0e-05\n",
      "\n",
      "Epoch 24: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.\n",
      "Epoch 25/50 | Time: 1753526766.29s | Loss: 0.0527 | Acc: 0.9913 | Val Loss: 0.6639 | Val Acc: 0.8165 | LR: 8.0e-06\n",
      "Epoch 26/50 | Time: 1753526770.56s | Loss: 0.0523 | Acc: 0.9908 | Val Loss: 0.6891 | Val Acc: 0.8090 | LR: 8.0e-06\n",
      "Epoch 27/50 | Time: 1753526774.83s | Loss: 0.0519 | Acc: 0.9900 | Val Loss: 0.6810 | Val Acc: 0.8125 | LR: 8.0e-06\n",
      "Epoch 28/50 | Time: 1753526779.13s | Loss: 0.0529 | Acc: 0.9898 | Val Loss: 0.6586 | Val Acc: 0.8165 | LR: 8.0e-06\n",
      "Epoch 29/50 | Time: 1753526783.38s | Loss: 0.0468 | Acc: 0.9933 | Val Loss: 0.6815 | Val Acc: 0.8085 | LR: 8.0e-06\n",
      "\n",
      "Epoch 29: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.\n",
      "Epoch 30/50 | Time: 1753526787.66s | Loss: 0.0499 | Acc: 0.9922 | Val Loss: 0.6783 | Val Acc: 0.8100 | LR: 1.6e-06\n",
      "Epoch 31/50 | Time: 1753526791.97s | Loss: 0.0495 | Acc: 0.9922 | Val Loss: 0.6756 | Val Acc: 0.8115 | LR: 1.6e-06\n",
      "Epoch 32/50 | Time: 1753526796.28s | Loss: 0.0504 | Acc: 0.9922 | Val Loss: 0.6764 | Val Acc: 0.8150 | LR: 1.6e-06\n",
      "Epoch 33/50 | Time: 1753526800.51s | Loss: 0.0521 | Acc: 0.9888 | Val Loss: 0.6731 | Val Acc: 0.8155 | LR: 1.6e-06\n",
      "Epoch 34/50 | Time: 1753526804.79s | Loss: 0.0478 | Acc: 0.9920 | Val Loss: 0.6766 | Val Acc: 0.8135 | LR: 1.6e-06\n",
      "\n",
      "Epoch 34: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.\n",
      "üèÅ Finished training. Best Validation Accuracy: 0.8205\n",
      "\n",
      "================================================================================\n",
      "TRAINING ARCHITECTURE: 'ResSE_AudioCNN'\n",
      "================================================================================\n",
      "üöÄ Starting training for model: UNet_Audio_Classifier...\n",
      "Epoch 01/50 | Time: 1753526821.97s | Loss: 1.4593 | Acc: 0.4765 | Val Loss: 7.3338 | Val Acc: 0.1760 | LR: 1.0e-03 ‚úÖ\n",
      "Epoch 02/50 | Time: 1753526826.33s | Loss: 1.0521 | Acc: 0.6362 | Val Loss: 2.5921 | Val Acc: 0.2150 | LR: 1.0e-03 ‚úÖ\n",
      "Epoch 03/50 | Time: 1753526830.74s | Loss: 0.8457 | Acc: 0.7160 | Val Loss: 2.6952 | Val Acc: 0.2670 | LR: 1.0e-03 ‚úÖ\n",
      "Epoch 04/50 | Time: 1753526835.19s | Loss: 0.6864 | Acc: 0.7740 | Val Loss: 3.1663 | Val Acc: 0.3330 | LR: 1.0e-03 ‚úÖ\n",
      "Epoch 05/50 | Time: 1753526839.56s | Loss: 0.5812 | Acc: 0.8068 | Val Loss: 2.0414 | Val Acc: 0.4960 | LR: 1.0e-03 ‚úÖ\n",
      "Epoch 06/50 | Time: 1753526844.00s | Loss: 0.4820 | Acc: 0.8429 | Val Loss: 1.3887 | Val Acc: 0.5785 | LR: 1.0e-03 ‚úÖ\n",
      "Epoch 07/50 | Time: 1753526848.43s | Loss: 0.4062 | Acc: 0.8643 | Val Loss: 4.7684 | Val Acc: 0.3860 | LR: 1.0e-03\n",
      "Epoch 08/50 | Time: 1753526852.79s | Loss: 0.3368 | Acc: 0.8922 | Val Loss: 3.4066 | Val Acc: 0.4615 | LR: 1.0e-03\n",
      "Epoch 09/50 | Time: 1753526857.02s | Loss: 0.3000 | Acc: 0.8995 | Val Loss: 4.8994 | Val Acc: 0.3770 | LR: 1.0e-03\n",
      "Epoch 10/50 | Time: 1753526861.32s | Loss: 0.2346 | Acc: 0.9290 | Val Loss: 4.9750 | Val Acc: 0.3370 | LR: 1.0e-03\n",
      "Epoch 11/50 | Time: 1753526865.63s | Loss: 0.2018 | Acc: 0.9377 | Val Loss: 2.1616 | Val Acc: 0.5415 | LR: 1.0e-03\n",
      "\n",
      "Epoch 11: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "Epoch 12/50 | Time: 1753526869.92s | Loss: 0.1211 | Acc: 0.9666 | Val Loss: 0.7390 | Val Acc: 0.7760 | LR: 2.0e-04 ‚úÖ\n",
      "Epoch 13/50 | Time: 1753526874.32s | Loss: 0.0870 | Acc: 0.9801 | Val Loss: 0.7589 | Val Acc: 0.7810 | LR: 2.0e-04 ‚úÖ\n",
      "Epoch 14/50 | Time: 1753526878.80s | Loss: 0.0723 | Acc: 0.9843 | Val Loss: 0.7630 | Val Acc: 0.7715 | LR: 2.0e-04\n",
      "Epoch 15/50 | Time: 1753526883.09s | Loss: 0.0681 | Acc: 0.9850 | Val Loss: 0.9186 | Val Acc: 0.7515 | LR: 2.0e-04\n",
      "Epoch 16/50 | Time: 1753526887.45s | Loss: 0.0626 | Acc: 0.9855 | Val Loss: 0.6913 | Val Acc: 0.7850 | LR: 2.0e-04 ‚úÖ\n",
      "Epoch 17/50 | Time: 1753526891.90s | Loss: 0.0577 | Acc: 0.9875 | Val Loss: 0.6800 | Val Acc: 0.7940 | LR: 2.0e-04 ‚úÖ\n",
      "Epoch 18/50 | Time: 1753526896.32s | Loss: 0.0540 | Acc: 0.9870 | Val Loss: 0.9288 | Val Acc: 0.7780 | LR: 2.0e-04\n",
      "Epoch 19/50 | Time: 1753526900.55s | Loss: 0.0547 | Acc: 0.9870 | Val Loss: 1.0078 | Val Acc: 0.7425 | LR: 2.0e-04\n",
      "Epoch 20/50 | Time: 1753526904.87s | Loss: 0.0455 | Acc: 0.9923 | Val Loss: 0.8124 | Val Acc: 0.7870 | LR: 2.0e-04\n",
      "Epoch 21/50 | Time: 1753526909.13s | Loss: 0.0420 | Acc: 0.9917 | Val Loss: 1.1578 | Val Acc: 0.7465 | LR: 2.0e-04\n",
      "Epoch 22/50 | Time: 1753526913.44s | Loss: 0.0404 | Acc: 0.9923 | Val Loss: 0.9219 | Val Acc: 0.7715 | LR: 2.0e-04\n",
      "\n",
      "Epoch 22: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
      "Epoch 23/50 | Time: 1753526917.79s | Loss: 0.0334 | Acc: 0.9948 | Val Loss: 0.7068 | Val Acc: 0.8110 | LR: 4.0e-05 ‚úÖ\n",
      "Epoch 24/50 | Time: 1753526922.23s | Loss: 0.0332 | Acc: 0.9953 | Val Loss: 0.7407 | Val Acc: 0.8095 | LR: 4.0e-05\n",
      "Epoch 25/50 | Time: 1753526926.54s | Loss: 0.0298 | Acc: 0.9965 | Val Loss: 0.7293 | Val Acc: 0.8075 | LR: 4.0e-05\n",
      "Epoch 26/50 | Time: 1753526930.83s | Loss: 0.0279 | Acc: 0.9950 | Val Loss: 0.7080 | Val Acc: 0.8110 | LR: 4.0e-05\n",
      "Epoch 27/50 | Time: 1753526935.14s | Loss: 0.0275 | Acc: 0.9962 | Val Loss: 0.6745 | Val Acc: 0.8185 | LR: 4.0e-05 ‚úÖ\n",
      "Epoch 28/50 | Time: 1753526939.45s | Loss: 0.0269 | Acc: 0.9963 | Val Loss: 0.7151 | Val Acc: 0.8105 | LR: 4.0e-05\n",
      "Epoch 29/50 | Time: 1753526943.73s | Loss: 0.0264 | Acc: 0.9960 | Val Loss: 0.7191 | Val Acc: 0.8040 | LR: 4.0e-05\n",
      "Epoch 30/50 | Time: 1753526948.01s | Loss: 0.0275 | Acc: 0.9958 | Val Loss: 0.6924 | Val Acc: 0.8165 | LR: 4.0e-05\n",
      "Epoch 31/50 | Time: 1753526952.32s | Loss: 0.0243 | Acc: 0.9970 | Val Loss: 0.7174 | Val Acc: 0.8060 | LR: 4.0e-05\n",
      "Epoch 32/50 | Time: 1753526956.62s | Loss: 0.0283 | Acc: 0.9952 | Val Loss: 0.7418 | Val Acc: 0.8060 | LR: 4.0e-05\n",
      "\n",
      "Epoch 32: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.\n",
      "Epoch 33/50 | Time: 1753526960.91s | Loss: 0.0235 | Acc: 0.9972 | Val Loss: 0.6758 | Val Acc: 0.8225 | LR: 8.0e-06 ‚úÖ\n",
      "Epoch 34/50 | Time: 1753526965.31s | Loss: 0.0225 | Acc: 0.9967 | Val Loss: 0.6737 | Val Acc: 0.8215 | LR: 8.0e-06\n",
      "Epoch 35/50 | Time: 1753526969.57s | Loss: 0.0213 | Acc: 0.9973 | Val Loss: 0.6703 | Val Acc: 0.8215 | LR: 8.0e-06\n",
      "Epoch 36/50 | Time: 1753526973.89s | Loss: 0.0234 | Acc: 0.9973 | Val Loss: 0.6621 | Val Acc: 0.8260 | LR: 8.0e-06 ‚úÖ\n",
      "Epoch 37/50 | Time: 1753526978.39s | Loss: 0.0223 | Acc: 0.9972 | Val Loss: 0.6790 | Val Acc: 0.8235 | LR: 8.0e-06\n",
      "Epoch 38/50 | Time: 1753526982.63s | Loss: 0.0214 | Acc: 0.9978 | Val Loss: 0.6661 | Val Acc: 0.8275 | LR: 8.0e-06 ‚úÖ\n",
      "Epoch 39/50 | Time: 1753526987.00s | Loss: 0.0206 | Acc: 0.9978 | Val Loss: 0.6724 | Val Acc: 0.8230 | LR: 8.0e-06\n",
      "Epoch 40/50 | Time: 1753526991.30s | Loss: 0.0237 | Acc: 0.9963 | Val Loss: 0.6738 | Val Acc: 0.8245 | LR: 8.0e-06\n",
      "Epoch 41/50 | Time: 1753526995.65s | Loss: 0.0225 | Acc: 0.9965 | Val Loss: 0.6799 | Val Acc: 0.8230 | LR: 8.0e-06\n",
      "\n",
      "Epoch 41: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.\n",
      "Epoch 42/50 | Time: 1753526999.93s | Loss: 0.0203 | Acc: 0.9982 | Val Loss: 0.6786 | Val Acc: 0.8235 | LR: 1.6e-06\n",
      "Epoch 43/50 | Time: 1753527004.19s | Loss: 0.0208 | Acc: 0.9980 | Val Loss: 0.6774 | Val Acc: 0.8215 | LR: 1.6e-06\n",
      "Epoch 44/50 | Time: 1753527008.50s | Loss: 0.0202 | Acc: 0.9970 | Val Loss: 0.6787 | Val Acc: 0.8210 | LR: 1.6e-06\n",
      "Epoch 45/50 | Time: 1753527012.78s | Loss: 0.0189 | Acc: 0.9977 | Val Loss: 0.6760 | Val Acc: 0.8200 | LR: 1.6e-06\n",
      "Epoch 46/50 | Time: 1753527017.06s | Loss: 0.0242 | Acc: 0.9960 | Val Loss: 0.6743 | Val Acc: 0.8220 | LR: 1.6e-06\n",
      "\n",
      "Epoch 46: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.\n",
      "Epoch 47/50 | Time: 1753527021.32s | Loss: 0.0226 | Acc: 0.9968 | Val Loss: 0.6758 | Val Acc: 0.8205 | LR: 3.2e-07\n",
      "Epoch 48/50 | Time: 1753527025.61s | Loss: 0.0201 | Acc: 0.9978 | Val Loss: 0.6761 | Val Acc: 0.8215 | LR: 3.2e-07\n",
      "Epoch 49/50 | Time: 1753527029.89s | Loss: 0.0220 | Acc: 0.9963 | Val Loss: 0.6762 | Val Acc: 0.8205 | LR: 3.2e-07\n",
      "Epoch 50/50 | Time: 1753527034.13s | Loss: 0.0193 | Acc: 0.9975 | Val Loss: 0.6760 | Val Acc: 0.8215 | LR: 3.2e-07\n",
      "üèÅ Finished training. Best Validation Accuracy: 0.8275\n",
      "\n",
      "================================================================================\n",
      "TRAINING ARCHITECTURE: 'UNet_Audio_Classifier'\n",
      "================================================================================\n",
      "üöÄ Starting training for model: UNet_Audio_Classifier...\n",
      "Epoch 01/50 | Time: 1753527049.42s | Loss: 1.5177 | Acc: 0.4589 | Val Loss: 6.7847 | Val Acc: 0.1465 | LR: 1.0e-03 ‚úÖ\n",
      "Epoch 02/50 | Time: 1753527054.20s | Loss: 1.1142 | Acc: 0.6067 | Val Loss: 5.5838 | Val Acc: 0.1435 | LR: 1.0e-03\n",
      "Epoch 03/50 | Time: 1753527058.47s | Loss: 0.8776 | Acc: 0.7027 | Val Loss: 4.8714 | Val Acc: 0.1845 | LR: 1.0e-03 ‚úÖ\n",
      "Epoch 04/50 | Time: 1753527062.81s | Loss: 0.7433 | Acc: 0.7504 | Val Loss: 3.1333 | Val Acc: 0.3865 | LR: 1.0e-03 ‚úÖ\n",
      "Epoch 05/50 | Time: 1753527067.12s | Loss: 0.6065 | Acc: 0.8000 | Val Loss: 2.6153 | Val Acc: 0.3710 | LR: 1.0e-03\n",
      "Epoch 06/50 | Time: 1753527071.40s | Loss: 0.5142 | Acc: 0.8232 | Val Loss: 1.7091 | Val Acc: 0.5515 | LR: 1.0e-03 ‚úÖ\n",
      "Epoch 07/50 | Time: 1753527075.75s | Loss: 0.4418 | Acc: 0.8559 | Val Loss: 3.3737 | Val Acc: 0.3765 | LR: 1.0e-03\n",
      "Epoch 08/50 | Time: 1753527080.04s | Loss: 0.3603 | Acc: 0.8845 | Val Loss: 1.8104 | Val Acc: 0.5875 | LR: 1.0e-03 ‚úÖ\n",
      "Epoch 09/50 | Time: 1753527084.46s | Loss: 0.2961 | Acc: 0.9100 | Val Loss: 2.8999 | Val Acc: 0.4495 | LR: 1.0e-03\n",
      "Epoch 10/50 | Time: 1753527088.75s | Loss: 0.2697 | Acc: 0.9130 | Val Loss: 1.3681 | Val Acc: 0.6575 | LR: 1.0e-03 ‚úÖ\n",
      "Epoch 11/50 | Time: 1753527093.08s | Loss: 0.2324 | Acc: 0.9245 | Val Loss: 1.7970 | Val Acc: 0.5675 | LR: 1.0e-03\n",
      "Epoch 12/50 | Time: 1753527097.40s | Loss: 0.1870 | Acc: 0.9386 | Val Loss: 1.9530 | Val Acc: 0.5495 | LR: 1.0e-03\n",
      "Epoch 13/50 | Time: 1753527101.68s | Loss: 0.1694 | Acc: 0.9482 | Val Loss: 1.0462 | Val Acc: 0.6825 | LR: 1.0e-03 ‚úÖ\n",
      "Epoch 14/50 | Time: 1753527106.05s | Loss: 0.1457 | Acc: 0.9553 | Val Loss: 3.3370 | Val Acc: 0.4905 | LR: 1.0e-03\n",
      "Epoch 15/50 | Time: 1753527110.35s | Loss: 0.1450 | Acc: 0.9551 | Val Loss: 4.0286 | Val Acc: 0.4780 | LR: 1.0e-03\n",
      "Epoch 16/50 | Time: 1753527114.66s | Loss: 0.0935 | Acc: 0.9711 | Val Loss: 4.8967 | Val Acc: 0.4475 | LR: 1.0e-03\n",
      "Epoch 17/50 | Time: 1753527118.91s | Loss: 0.0988 | Acc: 0.9711 | Val Loss: 3.4937 | Val Acc: 0.5475 | LR: 1.0e-03\n",
      "Epoch 18/50 | Time: 1753527123.23s | Loss: 0.0906 | Acc: 0.9684 | Val Loss: 5.1128 | Val Acc: 0.4060 | LR: 1.0e-03\n",
      "\n",
      "Epoch 18: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "Epoch 19/50 | Time: 1753527127.52s | Loss: 0.0498 | Acc: 0.9878 | Val Loss: 0.9368 | Val Acc: 0.7850 | LR: 2.0e-04 ‚úÖ\n",
      "Epoch 20/50 | Time: 1753527131.93s | Loss: 0.0289 | Acc: 0.9945 | Val Loss: 0.7496 | Val Acc: 0.8160 | LR: 2.0e-04 ‚úÖ\n",
      "Epoch 21/50 | Time: 1753527136.37s | Loss: 0.0282 | Acc: 0.9940 | Val Loss: 0.7497 | Val Acc: 0.8150 | LR: 2.0e-04\n",
      "Epoch 22/50 | Time: 1753527140.67s | Loss: 0.0260 | Acc: 0.9945 | Val Loss: 0.7782 | Val Acc: 0.7965 | LR: 2.0e-04\n",
      "Epoch 23/50 | Time: 1753527144.98s | Loss: 0.0188 | Acc: 0.9977 | Val Loss: 0.7752 | Val Acc: 0.8010 | LR: 2.0e-04\n",
      "Epoch 24/50 | Time: 1753527149.29s | Loss: 0.0196 | Acc: 0.9965 | Val Loss: 0.8237 | Val Acc: 0.8140 | LR: 2.0e-04\n",
      "Epoch 25/50 | Time: 1753527153.61s | Loss: 0.0193 | Acc: 0.9967 | Val Loss: 1.1791 | Val Acc: 0.7640 | LR: 2.0e-04\n",
      "\n",
      "Epoch 25: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
      "Epoch 26/50 | Time: 1753527157.89s | Loss: 0.0160 | Acc: 0.9978 | Val Loss: 0.8126 | Val Acc: 0.8170 | LR: 4.0e-05 ‚úÖ\n",
      "Epoch 27/50 | Time: 1753527162.37s | Loss: 0.0145 | Acc: 0.9978 | Val Loss: 0.7542 | Val Acc: 0.8185 | LR: 4.0e-05 ‚úÖ\n",
      "Epoch 28/50 | Time: 1753527166.75s | Loss: 0.0174 | Acc: 0.9968 | Val Loss: 0.7482 | Val Acc: 0.8200 | LR: 4.0e-05 ‚úÖ\n",
      "Epoch 29/50 | Time: 1753527171.13s | Loss: 0.0161 | Acc: 0.9972 | Val Loss: 0.7491 | Val Acc: 0.8220 | LR: 4.0e-05 ‚úÖ\n",
      "Epoch 30/50 | Time: 1753527175.59s | Loss: 0.0140 | Acc: 0.9983 | Val Loss: 0.7211 | Val Acc: 0.8270 | LR: 4.0e-05 ‚úÖ\n",
      "Epoch 31/50 | Time: 1753527179.98s | Loss: 0.0148 | Acc: 0.9968 | Val Loss: 0.7321 | Val Acc: 0.8240 | LR: 4.0e-05\n",
      "Epoch 32/50 | Time: 1753527184.28s | Loss: 0.0152 | Acc: 0.9972 | Val Loss: 0.7803 | Val Acc: 0.8125 | LR: 4.0e-05\n",
      "Epoch 33/50 | Time: 1753527188.58s | Loss: 0.0164 | Acc: 0.9973 | Val Loss: 0.7670 | Val Acc: 0.8235 | LR: 4.0e-05\n",
      "Epoch 34/50 | Time: 1753527192.92s | Loss: 0.0136 | Acc: 0.9985 | Val Loss: 0.8145 | Val Acc: 0.8150 | LR: 4.0e-05\n",
      "Epoch 35/50 | Time: 1753527197.21s | Loss: 0.0143 | Acc: 0.9975 | Val Loss: 0.7618 | Val Acc: 0.8125 | LR: 4.0e-05\n",
      "\n",
      "Epoch 35: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.\n",
      "Epoch 36/50 | Time: 1753527201.51s | Loss: 0.0132 | Acc: 0.9975 | Val Loss: 0.7647 | Val Acc: 0.8175 | LR: 8.0e-06\n",
      "Epoch 37/50 | Time: 1753527205.83s | Loss: 0.0121 | Acc: 0.9982 | Val Loss: 0.7517 | Val Acc: 0.8215 | LR: 8.0e-06\n",
      "Epoch 38/50 | Time: 1753527210.12s | Loss: 0.0133 | Acc: 0.9980 | Val Loss: 0.7474 | Val Acc: 0.8220 | LR: 8.0e-06\n",
      "Epoch 39/50 | Time: 1753527214.50s | Loss: 0.0115 | Acc: 0.9983 | Val Loss: 0.7502 | Val Acc: 0.8235 | LR: 8.0e-06\n",
      "Epoch 40/50 | Time: 1753527218.82s | Loss: 0.0140 | Acc: 0.9987 | Val Loss: 0.7556 | Val Acc: 0.8235 | LR: 8.0e-06\n",
      "\n",
      "Epoch 40: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.\n",
      "Epoch 41/50 | Time: 1753527223.12s | Loss: 0.0140 | Acc: 0.9977 | Val Loss: 0.7566 | Val Acc: 0.8245 | LR: 1.6e-06\n",
      "Epoch 42/50 | Time: 1753527227.38s | Loss: 0.0134 | Acc: 0.9973 | Val Loss: 0.7574 | Val Acc: 0.8255 | LR: 1.6e-06\n",
      "Epoch 43/50 | Time: 1753527231.72s | Loss: 0.0137 | Acc: 0.9975 | Val Loss: 0.7580 | Val Acc: 0.8265 | LR: 1.6e-06\n",
      "Epoch 44/50 | Time: 1753527236.05s | Loss: 0.0116 | Acc: 0.9978 | Val Loss: 0.7574 | Val Acc: 0.8255 | LR: 1.6e-06\n",
      "Epoch 45/50 | Time: 1753527240.34s | Loss: 0.0128 | Acc: 0.9978 | Val Loss: 0.7602 | Val Acc: 0.8255 | LR: 1.6e-06\n",
      "\n",
      "Epoch 45: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.\n",
      "üèÅ Finished training. Best Validation Accuracy: 0.8270\n",
      "\n",
      "üéâ FINAL COMPARATIVE ANALYSIS COMPLETED üéâ\n",
      "\n",
      "Final Leaderboard:\n",
      "| Model                 |   Test_Accuracy |   Best_Val_Accuracy |   Epochs_Run |\n",
      "|:----------------------|----------------:|--------------------:|-------------:|\n",
      "| Efficient_VGG         |          0.8205 |              0.8375 |           43 |\n",
      "| PaperCNN_Lite         |          0.8105 |              0.831  |           39 |\n",
      "| ResSE_AudioCNN        |          0.8175 |              0.8275 |           50 |\n",
      "| UNet_Audio_Classifier |          0.834  |              0.827  |           45 |\n",
      "| SeparableResSE_CNN    |          0.8145 |              0.8205 |           34 |\n",
      "| SE_AudioCNN           |          0.822  |              0.816  |           41 |\n"
     ]
    }
   ],
   "source": [
    "# ===================================================================\n",
    "# CELL 3: DEFINITIVE COMPARATIVE ANALYSIS FRAMEWORK\n",
    "# ===================================================================\n",
    "# This cell orchestrates the final comparative analysis. It loads the\n",
    "# pre-processed data, defines robust data pipelines with corrected\n",
    "# augmentation, and systematically trains and evaluates all candidate\n",
    "# architectures using a professional custom logger for clear progress tracking.\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import traceback\n",
    "import numpy as np\n",
    "import pickle\n",
    "import time\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import optimizers, callbacks\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 0. DATA LOADING AND PREPARATION\n",
    "# -------------------------------------------------------------------\n",
    "print(\"Loading pre-processed data...\")\n",
    "try:\n",
    "    X_train = np.load(os.path.join(PROCESSED_DATA_PATH, 'X_train.npy'))\n",
    "    y_train = np.load(os.path.join(PROCESSED_DATA_PATH, 'y_train.npy'))\n",
    "    X_val = np.load(os.path.join(PROCESSED_DATA_PATH, 'X_val.npy'))\n",
    "    y_val = np.load(os.path.join(PROCESSED_DATA_PATH, 'y_val.npy'))\n",
    "    X_test = np.load(os.path.join(PROCESSED_DATA_PATH, 'X_test.npy'))\n",
    "    y_test = np.load(os.path.join(PROCESSED_DATA_PATH, 'y_test.npy'))\n",
    "    with open(os.path.join(PROCESSED_DATA_PATH, 'label_encoder.pkl'), 'rb') as f:\n",
    "        label_encoder = pickle.load(f)\n",
    "\n",
    "    y_train_cat = to_categorical(y_train)\n",
    "    y_val_cat = to_categorical(y_val)\n",
    "    y_test_cat = to_categorical(y_test)\n",
    "    \n",
    "    print(\"‚úÖ Data successfully loaded and prepared.\")\n",
    "except FileNotFoundError:\n",
    "    raise RuntimeError(\"ERROR: Data files not found. Please run the '00_Data_Preprocessing' notebook first.\")\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 1. DATA AUGMENTATION & CUSTOM CALLBACK\n",
    "# -------------------------------------------------------------------\n",
    "@tf.function\n",
    "def spec_augment_tf(spectrogram, label):\n",
    "    \"\"\"\n",
    "    Applies frequency and time masking to a spectrogram.\n",
    "    *** BUG FIX: Corrected variable names from 'mask_values' to the\n",
    "    *** correctly scoped 'mask_freq_values' and 'mask_time_values'.\n",
    "    \"\"\"\n",
    "    aug_spec = tf.identity(spectrogram)\n",
    "    \n",
    "    # Frequency Masking\n",
    "    freq_bins = tf.shape(aug_spec)[0]\n",
    "    f_param = tf.cast(tf.cast(freq_bins, tf.float32) * 0.2, tf.int32)\n",
    "    if f_param > 1:\n",
    "        f = tf.random.uniform(shape=(), minval=1, maxval=f_param, dtype=tf.int32)\n",
    "        f0 = tf.random.uniform(shape=(), minval=0, maxval=freq_bins - f, dtype=tf.int32)\n",
    "        mask_freq_values = tf.concat([tf.ones((f0,)), tf.zeros((f,)), tf.ones((freq_bins - f0 - f,))], axis=0)\n",
    "        freq_mask = tf.reshape(tf.cast(mask_freq_values, aug_spec.dtype), (freq_bins, 1, 1))\n",
    "        aug_spec *= freq_mask\n",
    "\n",
    "    # Time Masking\n",
    "    time_steps = tf.shape(aug_spec)[1]\n",
    "    t_param = tf.cast(tf.cast(time_steps, tf.float32) * 0.2, tf.int32)\n",
    "    if t_param > 1:\n",
    "        t = tf.random.uniform(shape=(), minval=1, maxval=t_param, dtype=tf.int32)\n",
    "        t0 = tf.random.uniform(shape=(), minval=0, maxval=time_steps - t, dtype=tf.int32)\n",
    "        mask_time_values = tf.concat([tf.ones((t0,)), tf.zeros((t,)), tf.ones((time_steps - t0 - t,))], axis=0)\n",
    "        time_mask = tf.reshape(tf.cast(mask_time_values, aug_spec.dtype), (1, time_steps, 1))\n",
    "        aug_spec *= time_mask\n",
    "        \n",
    "    return aug_spec, label\n",
    "\n",
    "class RichLoggerCallback(callbacks.Callback):\n",
    "    \"\"\"A custom Keras callback for clean, informative, and professional logging.\"\"\"\n",
    "    def __init__(self, total_epochs):\n",
    "        super().__init__()\n",
    "        self.total_epochs = total_epochs\n",
    "        self.best_val_accuracy = 0\n",
    "        self.epoch_start_time = 0\n",
    "\n",
    "    def on_train_begin(self, logs=None):\n",
    "        print(f\"üöÄ Starting training for model: {self.model.name}...\")\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        epoch_time = time.time() - self.epoch_start_time\n",
    "        lr = self.model.optimizer.learning_rate\n",
    "        \n",
    "        # Handle potential learning rate schedules\n",
    "        if isinstance(lr, tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "            lr = lr(self.model.optimizer.iterations)\n",
    "\n",
    "        # *** BUG FIX: Convert the learning rate variable to a numpy value before formatting.\n",
    "        lr_value = lr.numpy() if hasattr(lr, 'numpy') else lr\n",
    "\n",
    "        is_best = \"\"\n",
    "        if logs['val_accuracy'] > self.best_val_accuracy:\n",
    "            self.best_val_accuracy = logs['val_accuracy']\n",
    "            is_best = \" ‚úÖ\"\n",
    "\n",
    "        log_str = (\n",
    "            f\"Epoch {epoch + 1:02d}/{self.total_epochs} | \"\n",
    "            f\"Time: {epoch_time:.2f}s | \"\n",
    "            f\"Loss: {logs['loss']:.4f} | Acc: {logs['accuracy']:.4f} | \"\n",
    "            f\"Val Loss: {logs['val_loss']:.4f} | Val Acc: {logs['val_accuracy']:.4f} | \"\n",
    "            f\"LR: {lr_value:.1e}{is_best}\"\n",
    "        )\n",
    "        print(log_str)\n",
    "\n",
    "    def on_train_end(self, logs=None):\n",
    "        print(f\"üèÅ Finished training. Best Validation Accuracy: {self.best_val_accuracy:.4f}\")\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 2. EXPERIMENT ORCHESTRATION CLASS\n",
    "# -------------------------------------------------------------------\n",
    "class ModelEvaluator:\n",
    "    \"\"\"Orchestrates the training and evaluation of multiple models.\"\"\"\n",
    "    def __init__(self):\n",
    "        self.results = []\n",
    "\n",
    "    def run_experiments(self, model_factories, train_data, val_data, test_data, epochs):\n",
    "        for model_name, model_factory_fn in model_factories.items():\n",
    "            print(f\"\\n{'='*80}\\nTRAINING ARCHITECTURE: '{model_name}'\\n{'='*80}\")\n",
    "            try:\n",
    "                model = model_factory_fn()\n",
    "                optimizer = keras.optimizers.Adam(learning_rate=1e-3)\n",
    "                model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "                \n",
    "                callbacks_list = [\n",
    "                    RichLoggerCallback(total_epochs=epochs),\n",
    "                    callbacks.EarlyStopping(monitor='val_accuracy', patience=15, restore_best_weights=True, verbose=0),\n",
    "                    callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, verbose=1),\n",
    "                    callbacks.ModelCheckpoint(os.path.join(MODELS_PATH, f\"{model_name}_best.keras\"), \n",
    "                                              monitor='val_accuracy', save_best_only=True, verbose=0)\n",
    "                ]\n",
    "                \n",
    "                history = model.fit(train_data, epochs=epochs, validation_data=val_data, callbacks=callbacks_list, verbose=0)\n",
    "                \n",
    "                test_loss, test_acc = model.evaluate(test_data, verbose=0)\n",
    "                self.results.append({\n",
    "                    'Model': model_name,\n",
    "                    'Test_Accuracy': test_acc,\n",
    "                    'Best_Val_Accuracy': max(history.history['val_accuracy']),\n",
    "                    'Epochs_Run': len(history.history['val_accuracy']),\n",
    "                })\n",
    "            except Exception:\n",
    "                print(f\"‚ùå ERROR during training of [{model_name}]:\")\n",
    "                traceback.print_exc()\n",
    "        return pd.DataFrame(self.results)\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 3. EXPERIMENT CONFIGURATION & EXECUTION\n",
    "# -------------------------------------------------------------------\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 50\n",
    "\n",
    "keras.mixed_precision.set_global_policy('float32')\n",
    "\n",
    "# Data Pipelines (without .cache() to ensure no OOM errors)\n",
    "print(\"\\nConfiguring JIT data pipelines (without caching)...\")\n",
    "train_pipeline = (tf.data.Dataset.from_tensor_slices((X_train, y_train_cat)).shuffle(len(X_train))\n",
    "                  .map(spec_augment_tf, num_parallel_calls=AUTOTUNE).batch(BATCH_SIZE).prefetch(AUTOTUNE))\n",
    "val_pipeline = (tf.data.Dataset.from_tensor_slices((X_val, y_val_cat)).batch(BATCH_SIZE).prefetch(AUTOTUNE))\n",
    "test_pipeline = (tf.data.Dataset.from_tensor_slices((X_test, y_test_cat)).batch(BATCH_SIZE).prefetch(AUTOTUNE))\n",
    "\n",
    "# Define the models for the final comparative analysis\n",
    "input_shape = X_train.shape[1:]\n",
    "num_classes = y_train_cat.shape[1]\n",
    "model_factories = {model_name: lambda: ModelFactory.get_builder_by_name(model_name)(input_shape, num_classes) for model_name in ModelFactory.get_all_model_names()}\n",
    "\n",
    "# Filter the model factories\n",
    "to_train = ['UNet_Audio_Classifier']\n",
    "# model_factories = {model_name: model_factory for model_name, model_factory in model_factories.items() if model_name in to_train}\n",
    "\n",
    "\n",
    "# Execute the comparative analysis\n",
    "evaluator = ModelEvaluator()\n",
    "results_df = evaluator.run_experiments(\n",
    "    model_factories, train_pipeline, val_pipeline, test_pipeline, EPOCHS\n",
    ")\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 4. REPORTING\n",
    "# -------------------------------------------------------------------\n",
    "if not results_df.empty:\n",
    "    results_df.to_csv(os.path.join(REPORTS_PATH, 'training_summary.csv'), index=False)\n",
    "    print(\"\\nüéâ FINAL COMPARATIVE ANALYSIS COMPLETED üéâ\")\n",
    "    print(\"\\nFinal Leaderboard:\")\n",
    "    print(results_df.sort_values(by='Best_Val_Accuracy', ascending=False).to_markdown(index=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
